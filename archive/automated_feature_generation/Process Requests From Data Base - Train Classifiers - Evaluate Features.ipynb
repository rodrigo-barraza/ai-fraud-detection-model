{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Extract Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for numerical workimport pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pymongo\n",
    "\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "from pandas.io.json import json_normalize\n",
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use full width of browser.\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load database from Mongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the database credentials from file\n",
    "\n",
    "# SET APPROPRIATE PATH FOR THE 'creds.json' FILE.\n",
    "\n",
    "with open('../../../data-science-poc/user_aggregation_pipeline/creds.json') as json_data:\n",
    "    creds = json.load(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient(creds['connection_string'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Loading takes about 12 minutes.\n",
    "# If it seems to be taking too long make sure you're not on guest wifi. \n",
    "# Reset the wifi on the computer seems to fix the problem, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load using list comprehension.\n",
    "\n",
    "# request_event_sets = [{'request': rs['request'],'event': event} for rs in client['ml']['requestEvents60'].find() for event in rs['events']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Or load using explicit for loops and progress reporting.\n",
    "\n",
    "request_event_sets = []\n",
    "i = 0 # There's about 650,000 entries at present. Will print a count every 50,000.\n",
    "\n",
    "for rs in client['ml']['requestEvents60'].find():\n",
    "    #print(rs)\n",
    "    for event in rs['events']:\n",
    "        tmp = {'request': rs['request'],'event': event}\n",
    "        #print(tmp)\n",
    "        request_event_sets.append( tmp )\n",
    "        i +=1\n",
    "        if i % 50000 == 0:\n",
    "            print(i)\n",
    "#         if i == 10000: # To limit to only first N entries instead.\n",
    "#             raise StopIteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to data frame.\n",
    "\n",
    "df = json_normalize(request_event_sets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Append fraud variable to df and extract interac/buy (credit card) records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flagFraudsters(df):\n",
    "\n",
    "    # Extract the black list eamils.\n",
    "    # NOTE: This includes credit card emails that have been blocked. Not just interac.\n",
    "    blemails = list(pd.DataFrame(json_normalize(list(client['production']['emailBlacklistCollection'].find({'level': 'BLOCKED'})))).email) + ['gaelkevin@hotmail.com', 'royer.8383@gmail.com','adventurous7381@gmail.com']\n",
    "    \n",
    "    df['fraud'] = df['event.metadata.email'].isin(blemails).astype(int)\n",
    "    df['fraud'] = df['request.metadata.email'].isin(blemails).astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def removeWhitelistRecords(df):\n",
    "\n",
    "    # Intended for testing for the most part. Generally fingerfood accounts.\n",
    "    wlemails = pd.DataFrame(json_normalize(list(client['production']['emailWhitelistCollection'].find({'level': 'ALLOWED'})))).email\n",
    "    \n",
    "    df = df[df['event.metadata.email'].str.contains('test') == False]\n",
    "    df = df[df['event.metadata.email'].str.contains('fingerfoodstudios') == False]\n",
    "    df = df[df['event.metadata.email'].str.contains('einstein.exchange') == False]\n",
    "    df = df[df['event.metadata.email'].isin(wlemails) == False]\n",
    "\n",
    "    df = df[df['request.metadata.email'].str.contains('test') == False]\n",
    "    df = df[df['request.metadata.email'].str.contains('fingerfoodstudios') == False]\n",
    "    df = df[df['request.metadata.email'].str.contains('einstein.exchange') == False]\n",
    "    df = df[df['request.metadata.email'].isin(wlemails) == False]\n",
    "    \n",
    "    return df \n",
    "\n",
    "# flag the fraudulnet records and remove the whitelist and test accounts\n",
    "df = removeWhitelistRecords(flagFraudsters(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract either INTERAC or CREDIT CARD events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "request_category = 'buy' # Set to 'buy' (credit card) or 'interac' for Interac.\n",
    "\n",
    "if request_category == 'interac':\n",
    "    events_df = df[\n",
    "    (df['request.eventCategory'] == 'interac') &\n",
    "    (df['request.eventAction'] == 'request') # This is redundant right now. Here for completeness.\n",
    "    ].copy()\n",
    "\n",
    "elif request_category == 'buy':\n",
    "    events_df = df[\n",
    "    (df['request.eventCategory'] == 'buy') &\n",
    "    (df['request.eventAction'] == 'request') # This is redundant right now. Here for completeness.\n",
    "    ].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Cleaning and Transform Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Concatenate Lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Some columns are lists of strings.\n",
    "# For now just concatenate these into a single string and use as categorical variables.\n",
    "# This seemed a reasonable work around until the columns can be looked at further.\n",
    "\n",
    "# NOTE: The column cleaning pipeline assumes this operation has taken place in some of the column transformers.\n",
    "# The concatenated columns are returned in 'list_columns' if desired.\n",
    "\n",
    "def concatenate_lists_to_string( df ):\n",
    "\n",
    "    list_columns = []\n",
    "\n",
    "    for c in list( df.columns ):\n",
    "        try:\n",
    "            df[c].nunique() # This fails on list entries.\n",
    "        except TypeError:   # So on fail convert the list of strings to a single string.\n",
    "            list_columns.append(c)    \n",
    "            df[c] = df[c].apply( lambda x: str(x) ).replace(['nan'],np.nan)\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "events_df = concatenate_lists_to_string( events_df )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Cleaning Pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from define_cleaning_map import map_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "clean_events_df = map_clean.fit_transform(events_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Restore the 'event/request.created' columns to the original datetime format. This fixes the accidental conversion to ns.\n",
    "# For some reason this doesn't happen when we scale the float columns in the scaling that follows. The datetime columns pass through correctly.\n",
    "\n",
    "clean_events_df['event.created'] = pd.to_datetime( events_df['event.created'] )\n",
    "clean_events_df['request.created'] = pd.to_datetime( events_df['request.created'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Do not need 'event._id'. NEED TO UPDATE THE CLEANING FILE TYPES .PY FILE TO DO THIS AUTOMATICALLY.\n",
    "\n",
    "clean_events_df.drop('event._id', axis=1, inplace=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Extract Null/Non-Null Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "clean_null_cols = []\n",
    "\n",
    "tmp_df = clean_events_df[clean_events_df['request.eventCategory'] == 'buy']\n",
    "\n",
    "for c in tmp_df.columns:\n",
    "    if tmp_df[c].isnull().all():\n",
    "        #print(c)\n",
    "        clean_null_cols.append(c)\n",
    "\n",
    "len(clean_null_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "all_cols = clean_events_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "clean_nonnull_cols = list( set(all_cols) - set(clean_null_cols) )\n",
    "len( clean_nonnull_cols )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "clean_events_df = clean_events_df[ clean_nonnull_cols ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Do Scaling Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from define_scaling_map import map_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "scale_events_df = map_scale.fit_transform(clean_events_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Feature Tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Force data types for Feature Tools to recognize automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from define_column_variable_types import (\n",
    "    categorical_cols,\n",
    "    datetime_cols,\n",
    "    float_cols,\n",
    "    id_cols,\n",
    "    target_col,\n",
    "    request_cols\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in categorical_cols:\n",
    "    if c in scale_events_df.columns:\n",
    "        scale_events_df[c] = scale_events_df[c].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in float_cols:\n",
    "    if c in scale_events_df.columns:\n",
    "        scale_events_df[c] = scale_events_df[c].astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in id_cols:\n",
    "    if c in scale_events_df.columns:\n",
    "        scale_events_df[c] = scale_events_df[c].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_events_df['fraud'] = pd.to_numeric( scale_events_df['fraud'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "request_keep_cols = []\n",
    "for c in request_cols:\n",
    "    if c in scale_events_df.columns and c != 'request._id': # Keep 'request.id' out of the columns to keep. It will be inherited as an index when 'requests' es is normalized from the 'events' es.\n",
    "        request_keep_cols.append(c)\n",
    "        \n",
    "request_keep_cols.append('fraud')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now run Feature Tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import featuretools as ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = ft.EntitySet('user_events')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "es.entity_from_dataframe(entity_id='events',\n",
    "                        index='Event_ID',  # NOTE: Because of the hour back-window used 'event._id' is no longer unique. Can be assigned to multiple requests.\n",
    "                        make_index=True,\n",
    "                        time_index='event.created',\n",
    "                        dataframe=scale_events_df\n",
    "#                       ,variable_types={} # Don't need. Take variable types from the data frame.\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es.normalize_entity(new_entity_id='requests',\n",
    "                   base_entity_id='events',\n",
    "                   index='request._id',\n",
    "                   time_index_reduce='last',\n",
    "                    # Keep 'fraud' for target variable, '...email' for grouping in CV.\n",
    "                   additional_variables=request_keep_cols) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_matrix, features = ft.dfs(entityset=es\n",
    "                                  ,target_entity=\"requests\",\n",
    "                                  agg_primitives=['sum', 'std', 'max', 'skew', 'min', 'mean', 'count', 'percent_true', 'num_unique', 'mode', 'time_since_last','avg_time_between']\n",
    "                                  #trans_primitives=['day'], # Use default.\n",
    "                                  ,ignore_variables={'requests': ['request.metadata.email']} # This will be used for grouping. Don't aggregate for a feature.\n",
    "                                  ,features_only=False\n",
    "                                  ,max_depth=3  # In this setup of es.events -> es.requests we go no more than three steps in the recursion anyway.\n",
    "                                  ,max_features=-1\n",
    "                                 ,verbose=2\n",
    "                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_matrix_enc, features_enc = ft.encode_features(feature_matrix, features,\n",
    "                                                     top_n=10,\n",
    "                                                     include_unknown=True, # \n",
    "                                                     verbose=True,\n",
    "                                                     to_encode=list(set(feature_matrix.columns).difference('request.metadata.email'))\n",
    "                                                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save features and feature matrix if required.\n",
    "\n",
    "# ft.save_features( features_enc, \"feature_enc-definitions\")\n",
    "\n",
    "# feature_matrix_enc.to_pickle('feature_matrix_enc.pkl')\n",
    "\n",
    "# !ls -al *.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load use this.\n",
    "\n",
    "# feature_matrix_enc = pd.read_pickle('feature_matrix_enc.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Classifiers on Fraud Requests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "import sklearn\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the NaN's with zeros for now.\n",
    "\n",
    "feature_matrix_enc['AVG_TIME_BETWEEN(events.event.created)'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature matrix.\n",
    "X = feature_matrix_enc.drop(['fraud'], axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels are fraud\n",
    "y = feature_matrix_enc['fraud'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = es['requests'].df['request.metadata.email']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data stratified by request so there's no cross contamination.\n",
    "# This models the production set up where fraud is detected on a request level basis.\n",
    "\n",
    "n_splits = 5\n",
    "\n",
    "group_kfold = GroupKFold(n_splits=n_splits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mdl = LogisticRegression(class_weight='balanced', penalty='l1', tol = 1e-4, C=1e-3, solver='saga')\n",
    "\n",
    "mdl = RandomForestClassifier(class_weight='balanced')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Regression Over the CV Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_precisions = []\n",
    "i=0\n",
    "\n",
    "X_shuffled, y_shuffled, groups_shuffled = shuffle(X, y, groups, random_state=10) # This doesn't seem to actually shuffle or randomize anything. Need to check.\n",
    "\n",
    "for train_index, test_index in group_kfold.split(X_shuffled, y_shuffled, groups_shuffled):\n",
    "    \n",
    "    i += 1\n",
    "    \n",
    "    X_train = X_shuffled[train_index,:]\n",
    "    X_test = X_shuffled[test_index,:]\n",
    "    y_train = y_shuffled[train_index]\n",
    "    y_test = y_shuffled[test_index]\n",
    "\n",
    "    mdl.fit(X_train, y_train)\n",
    "\n",
    "    preds_train = mdl.predict(X_train)\n",
    "    preds_test = mdl.predict(X_test)\n",
    "    probs_train = mdl.predict_proba(X_train)[:,1]\n",
    "    probs_test = mdl.predict_proba(X_test)[:,1]\n",
    "    \n",
    "#     training_accuracy = np.sum(preds_train == y_train)/len(y_train)\n",
    "#     testing_accuracy = np.sum(preds_test == y_test)/len(y_test)\n",
    "#     print(\"Training Accuracy:\", training_accuracy)\n",
    "#     print(\"Testing Accuracy:\", testing_accuracy)\n",
    "    \n",
    "    training_avp = sklearn.metrics.average_precision_score(y_train, probs_train, average='weighted')\n",
    "    testing_avp = sklearn.metrics.average_precision_score(y_test, probs_test, average='weighted')\n",
    "    print(\"Training Average Precision:\", training_avp)\n",
    "    print(\"Testing Average Precision:\", testing_avp)\n",
    "    \n",
    "#     CM_train = sklearn.metrics.confusion_matrix(y_train, preds_train, sample_weight=None)\n",
    "#     print(CM_train)\n",
    "#     CM_test = sklearn.metrics.confusion_matrix(y_test, preds_test, sample_weight=None)\n",
    "#     print(CM_test)\n",
    "\n",
    "    average_precision = average_precision_score(y_true=y_test, y_score=probs_test, average='weighted')\n",
    "    average_precisions.append(average_precision)\n",
    "    \n",
    "    precision, recall, _ = precision_recall_curve(y_test, probs_test)\n",
    "    \n",
    "    pyplot.step(recall, precision, color='b', alpha=0.2,\n",
    "             where='post')\n",
    "    pyplot.fill_between(recall, precision, step='post', alpha=0.2,\n",
    "                     color='b')\n",
    "    pyplot.xlabel('Recall')\n",
    "    pyplot.ylabel('Precision')\n",
    "    pyplot.ylim([0.0, 1.05])\n",
    "    pyplot.xlim([0.0, 1.0])\n",
    "    pyplot.title('2-class Precision-Recall curve: AP={0:0.2f}'.format(\n",
    "        average_precision))\n",
    "    pyplot.show()\n",
    "\n",
    "print('==============================================')\n",
    "print( \"Mean Average Precision Score: {0:.2f}\".format(np.mean(average_precisions)) )\n",
    "print('==============================================')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FOR RANDOM FOREST --> Report Random Forest Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = mdl.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in mdl.estimators_],\n",
    "             axis=0)\n",
    "indices = np.argsort(importances)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(X.shape[1]):\n",
    "    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the feature importances of the forest\n",
    "plt.figure()\n",
    "plt.figure(figsize=(30,12))\n",
    "plt.title(\"Feature importances\")\n",
    "\n",
    "# plt.bar(range(X.shape[1]), importances[indices],\n",
    "#        color=\"r\")#, yerr=std[indices], align=\"center\")\n",
    "# plt.xticks(range(X.shape[1]), indices)\n",
    "# plt.xlim([-1, X.shape[1]])\n",
    "\n",
    "n = 20\n",
    "\n",
    "plt.bar(range(n), importances[indices[:n]],\n",
    "       color=\"r\", yerr=std[indices[:n]], align=\"center\")\n",
    "plt.xticks(range(n), indices[:n])\n",
    "plt.xlim([-1, n])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list = list( feature_matrix_enc.drop(['fraud'], axis=1).columns )\n",
    "\n",
    "feature_list[indices[0:n]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FOR LASSO LOGISTIC REGRESSION --> Perform Feature Stability Selection\n",
    "\n",
    "NOTE: This runs independently of the above one shot logistic regression code.\n",
    "This is more computationally intensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stability_selection import StabilitySelection, plot_stability_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_estimator = Pipeline([\n",
    "        ('scaler', StandardScaler()),  # GENERALLY YOU STANDARDIZE COLUMNS FOR LINEAR REGRESSION. TRY WITHOUT?\n",
    "        ('model', LogisticRegression(penalty='l1'))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: As is, this does not respect the email groups or the imbalanced fraud/OK ratio (~8%).\n",
    "# This is not cross-validation, however. It's multiple bootstrap sample of N/2 samples (nominally 100 times).\n",
    "# The 8% is probably large enough that the N/2 samples will be close enough to the minority/majority class ration.\n",
    "\n",
    "# The credit card data takes ~15-20 minutes for 10 lambda values.\n",
    "# This gives a sketch of the stability curves and lets you peg the lambda min/max values.\n",
    "# 50 lambdas give acceptable curves and take ~1 hour and a bit.\n",
    "    \n",
    "selector = StabilitySelection(base_estimator=base_estimator, lambda_name='model__C',\n",
    "                              lambda_grid=np.logspace(-7, -2, 50),\n",
    "                              verbose=2,\n",
    "                              n_jobs=4) # NOTE: This _does_ take advantage of multi-core and it makes a difference. My Mac has 4 cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = .95\n",
    "\n",
    "fig, ax = plot_stability_path(selector, threshold_highlight=thresh )\n",
    "fig.show()\n",
    "#plt.figure(figsize=(30,12))\n",
    "selected_variables = selector.get_support(indices=True, threshold=thresh)\n",
    "selected_scores = selector.stability_scores_.max(axis=1)\n",
    "\n",
    "print('Selected variables are:')\n",
    "print('-----------------------')\n",
    "\n",
    "for idx, (variable, score) in enumerate(zip(selected_variables, selected_scores[selected_variables])):\n",
    "    print('Variable %d: [%d], score %.3f' % (idx + 1, variable, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in list(selected_variables):\n",
    "    print( i, ':   ', feature_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
