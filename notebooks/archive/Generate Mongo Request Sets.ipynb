{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for auto-reloading extensions - helpful if you're writing and testing a package\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# for inline plotting in python using matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# for easier plots - also makes matplotlib plots look nicer by default\n",
    "import seaborn as sns\n",
    "\n",
    "# set up for using plotly offline without an API key - great for interactive plots\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "import plotly.graph_objs as go\n",
    "import plotly.figure_factory as ff\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "# for numerical work\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pymongo\n",
    "\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "from pandas.io.json import json_normalize\n",
    "from pymongo import MongoClient\n",
    "\n",
    "import pickle\n",
    "\n",
    "# load the database credentials from file\n",
    "with open('../creds/creds.json') as json_data:\n",
    "    creds = json.load(json_data)\n",
    "    \n",
    "client = MongoClient(creds['connection_string'])\n",
    "\n",
    "print(\"Getting all interac and credit card requests from the eventCollection\")\n",
    "# get the full history of interac and credit card requests\n",
    "all_requests = list(client['production']['eventCollection'].find({\n",
    "        'eventCategory': {'$in': ['interac', 'buy']},\n",
    "        'eventAction': 'request',\n",
    "        'metadata.email': {'$ne': None}}))\n",
    "\n",
    "# get all the dates\n",
    "dates = [r['created'] for r in all_requests]\n",
    "\n",
    "# get the min and max dates\n",
    "max_date = max(dates)\n",
    "min_date = min(dates)\n",
    "\n",
    "# calculate the min and max search dates\n",
    "min_search_date = min_date - datetime.timedelta(seconds=60*60) # one hour before the first event\n",
    "max_search_date = max_date\n",
    "\n",
    "# get all the users\n",
    "users = list(set([r['metadata']['email'] for r in all_requests]))\n",
    "\n",
    "print(\"Removing requests by whitelist/test users\")\n",
    "\n",
    "def remove_whitelist_emails(user_list):\n",
    "    '''Remove whitelisted or test emails'''\n",
    "\n",
    "    # get all the events related to the requests aka within 60 minutes before the first request for the users who mader requests\n",
    "    whitelist_emails = [r['email'] for r in list(client['production']['emailWhitelistCollection'].find({'level': 'ALLOWED'}))]\n",
    " \n",
    "    return [user for user in user_list if not ((user in whitelist_emails) or ('test' in user) or ('einstein.exchange' in user) or ('fingerfoodstudios' in user))]\n",
    "\n",
    "# remove whitelist accounts\n",
    "clean_users = remove_whitelist_emails(users)\n",
    "\n",
    "# remove events from whitelist users\n",
    "all_requests = [r for r in all_requests if r['metadata']['email'] in clean_users]\n",
    "\n",
    "print(\"Getting all user events within 60 minutes of the requests\")\n",
    "\n",
    "# get all the events related to the requests aka within 60 minutes before the first request for the users who mader requests\n",
    "all_user_events = list(client['production']['eventCollection'].find({\n",
    "        'created': {'$gte': min_search_date, '$lte': max_search_date},\n",
    "        'metadata.email': {'$in': clean_users}}))\n",
    "\n",
    "# generate a list of user events for each user\n",
    "user_event_dict = {}\n",
    "\n",
    "def add_event(event):\n",
    "    '''Adds an event to the user_event_dict'''\n",
    "    \n",
    "    email = event['metadata']['email']\n",
    "    \n",
    "    if email in user_event_dict.keys():\n",
    "        user_event_dict[email].append(event)\n",
    "    else:\n",
    "        user_event_dict[email] = [event]\n",
    "\n",
    "_ = [add_event(event) for event in all_user_events]\n",
    "\n",
    "def get_request_user_events(request, all_user_events):\n",
    "    '''Gets all the user events that are within 60 minutes before the request'''\n",
    "    \n",
    "    email = request['metadata']['email']\n",
    "    time = request['created']\n",
    "    lookback = time - datetime.timedelta(seconds=60*60)\n",
    "    \n",
    "    events = [event for event in user_event_dict[email] if (event['created'] >= lookback) & (event['created'] <= time)]\n",
    "    \n",
    "    return {'request_id': request['_id'], 'request_created': time, 'request_email': email, 'events': events}\n",
    "\n",
    "\n",
    "print(\"Processing records into dataframe\")\n",
    "\n",
    "# generate the sets of events for each interac or credit card request\n",
    "request_sets = [get_request_user_events(request, all_user_events) for request in all_requests]\n",
    "\n",
    "# flatten into one object per event\n",
    "flat_requests = [{'request_id': rs['request_id'], 'request_created': rs['request_created'], 'request_email': rs['request_email'], 'event': event} for rs in request_sets for event in rs['events']]\n",
    "\n",
    "len(flat_requests)\n",
    "\n",
    "all_events = pd.DataFrame(json_normalize(flat_requests))\n",
    "\n",
    "all_events.columns = [col if 'event.' not in col else col.replace('event.','') for col in all_events.columns]\n",
    "\n",
    "# create a dataframe with the results\n",
    "df_with_id = all_events\n",
    "\n",
    "# sort by request id and date\n",
    "df_with_id = df_with_id.sort_values(by=['request_id','created'])\n",
    "\n",
    "# calculate the previous event time and the time between events\n",
    "df_with_id['previous_event_time'] = df_with_id.groupby(['_id'])['created'].shift(1)\n",
    "df_with_id['time_since_last_event'] = pd.to_numeric(df_with_id['created']-df_with_id['previous_event_time'])*1e-9\n",
    "\n",
    "# replace string versions of infinity with proper inf object\n",
    "df_with_id = df_with_id.replace('Infinity', np.inf)\n",
    "\n",
    "# convert columns that should be to numeric\n",
    "df_with_id['metadata.amount'] = pd.to_numeric(df_with_id['metadata.amount'])\n",
    "df_with_id['metadata.rate'] = pd.to_numeric(df_with_id['metadata.rate'])\n",
    "df_with_id['metadata.cents'] = pd.to_numeric(df_with_id['metadata.cents'])\n",
    "df_with_id['value'] = pd.to_numeric(df_with_id['value'])\n",
    "\n",
    "df_with_id.head()\n",
    "\n",
    "# choose the columns to keep\n",
    "columns_to_keep = ['request_id',\n",
    "                   'request_email', \n",
    "                   'created', \n",
    "                   'eventAction', \n",
    "                   'eventCategory', \n",
    "                   'eventLabel', \n",
    "                   'metadata.addressCity', \n",
    "                   'metadata.addressCountry', \n",
    "                   'metadata.addressProvince', \n",
    "                   'metadata.amount', \n",
    "                   'metadata.cents', \n",
    "                   'metadata.city', \n",
    "                   'metadata.country', \n",
    "                   'metadata.currency', \n",
    "                   'metadata.instrument', \n",
    "                   'metadata.lastTradedPx', \n",
    "                   'metadata.mongoResponse.amount',\n",
    "                   'metadata.mongoResponse.price', \n",
    "                   'metadata.mongoResponse.product', \n",
    "                   'metadata.price', \n",
    "                   'metadata.product', \n",
    "                   'metadata.prossessorError.billingDetails.city', \n",
    "                   'metadata.prossessorError.billingDetails.country', \n",
    "                   'metadata.prossessorError.billingDetails.state', \n",
    "                   'metadata.prossessorError.card.type', \n",
    "                   'metadata.prossessorResponse.billingDetails.city', \n",
    "                   'metadata.prossessorResponse.billingDetails.country', \n",
    "                   'metadata.prossessorResponse.billingDetails.province', \n",
    "                   'metadata.prossessorResponse.billingDetails.state', \n",
    "                   'metadata.prossessorResponse.card.type', \n",
    "                   #'metadata.prossessorResponse.cardType', \n",
    "                   'metadata.prossessorResponse.card_type', \n",
    "                   'metadata.prossessorResponse.charge_amount', \n",
    "                   'metadata.province', \n",
    "                   'metadata.rate', \n",
    "                   'metadata.requestParams.amount', \n",
    "                   'metadata.requestParams.charge_amount', \n",
    "                   'metadata.requestParams.currency', \n",
    "                   'metadata.requestParams.price', \n",
    "                   'metadata.requestParams.product', \n",
    "                   'metadata.requestParams.product_amount', \n",
    "                   'metadata.secondAmount', \n",
    "                   'metadata.tradesResponse', \n",
    "                   'metadata.type', \n",
    "                   #'previous_event_time',  \n",
    "                   'time_since_last_event', \n",
    "                   'value']\n",
    "\n",
    "# choose the columns to expand\n",
    "columns_to_expand = ['eventAction', \n",
    "                     'eventCategory', \n",
    "                     'eventLabel', \n",
    "                     'metadata.addressCity', \n",
    "                     'metadata.addressCountry', \n",
    "                     'metadata.addressProvince', \n",
    "                     'metadata.city', \n",
    "                     'metadata.country', \n",
    "                     'metadata.currency', \n",
    "                     'metadata.instrument', \n",
    "                     'metadata.mongoResponse.product', \n",
    "                     'metadata.product', \n",
    "                     'metadata.prossessorError.billingDetails.city', \n",
    "                     'metadata.prossessorError.billingDetails.country', \n",
    "                     'metadata.prossessorError.billingDetails.state', \n",
    "                     'metadata.prossessorError.card.type', \n",
    "                     'metadata.prossessorResponse.billingDetails.city', \n",
    "                     'metadata.prossessorResponse.billingDetails.country', \n",
    "                     'metadata.prossessorResponse.billingDetails.province', \n",
    "                     'metadata.prossessorResponse.billingDetails.state', \n",
    "                     'metadata.prossessorResponse.card.type', \n",
    "                     #'metadata.prossessorResponse.cardType', \n",
    "                     'metadata.prossessorResponse.card_type', \n",
    "                     'metadata.province', \n",
    "                     'metadata.requestParams.currency', \n",
    "                     'metadata.requestParams.product', \n",
    "                     'metadata.tradesResponse', \n",
    "                     'metadata.type']\n",
    "\n",
    "# get the subset of columns\n",
    "subset = df_with_id[columns_to_keep]\n",
    "\n",
    "# fill inf values with na        \n",
    "subset = subset.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# create columns to track na status of each column\n",
    "for column in subset.columns:\n",
    "    if column not in ['request_id','request_email','created']:\n",
    "        subset[column+\"_NaN\"] = subset[column].isna().astype(int)\n",
    "        \n",
    "# convert categorical columns to binary\n",
    "subset = pd.get_dummies(subset, columns=columns_to_expand)\n",
    "\n",
    "# fill na values with 0\n",
    "subset = subset.fillna(0)\n",
    "\n",
    "# convert the datetime to integer nanoseconds since 1970\n",
    "subset['created_utc_ns'] = pd.to_numeric(subset['created'])\n",
    "\n",
    "# sort ascending by request_id and descending by time\n",
    "subset = subset.sort_values(by=['request_id','created'], ascending=[True, False])\n",
    "\n",
    "# grab the emails\n",
    "user_emails = subset['request_email']\n",
    "\n",
    "# drop the time and email columns\n",
    "subset = subset.drop(['request_email','created'], axis = 1)\n",
    "subset = subset.reset_index(drop=True)\n",
    "\n",
    "# hacky way to number the sequence events prior to each interac request with 0 being the request and 10 being the 10th event prior to the request\n",
    "subset['index_int'] = subset.index\n",
    "event_index = subset.groupby('request_id')['index_int'].agg(lambda x: list(np.abs(min(x)-x)))\n",
    "all_index = [[item] if type(item) == type(int) else list(item) for item in event_index]\n",
    "all_index = [item for sublist in all_index for item in sublist]\n",
    "subset['timesteps'] = all_index\n",
    "\n",
    "ids = subset.request_id.unique()\n",
    "ints = np.arange(0, len(ids))\n",
    "\n",
    "request_id_mapper = dict(zip(ids, ints))\n",
    "\n",
    "# get the dimensions of the data\n",
    "n_examples = subset.request_id.unique().size\n",
    "n_timesteps = subset.timesteps.max()+1 # because the indexing starts at 0 so the event with index 10 is actually the 11th event\n",
    "n_features = subset.columns.drop(['request_id','index_int','timesteps']).size\n",
    "features = list(subset.columns.drop(['request_id','index_int','timesteps']))\n",
    "\n",
    "# create a boolean map to set the na_columns to 1\n",
    "na_col_map = np.array([i for i, col in enumerate(features) if '_NaN' in col])\n",
    "\n",
    "# set up the empty dataframe\n",
    "data = np.zeros((n_examples,n_timesteps,n_features))\n",
    "\n",
    "# initialize all the na cols to 1                      \n",
    "# data[:,:,na_col_map] = 1.0\n",
    "\n",
    "# get only the features\n",
    "data_df = subset[features]\n",
    "\n",
    "# for each row in the dataframe of request events\n",
    "for i in range(data_df.shape[0]):\n",
    "    \n",
    "    # print out a status every 1000 records\n",
    "    if i+1%1000 == 0:\n",
    "        print(i, 'events processed out of' data_df.shape[0])\n",
    "    # get the example index\n",
    "    example_id = int(request_id_mapper[subset.request_id.iloc[i]])\n",
    "    \n",
    "    # get the timestep index\n",
    "    timestep_id = int(subset.timesteps.iloc[i])\n",
    "    \n",
    "    # get the values index\n",
    "    values = data_df.iloc[i,:].values.astype('float64')\n",
    "    \n",
    "    # update the values of that example\n",
    "    data[example_id, timestep_id,:] = values\n",
    "\n",
    "print(\"Shape of dataset:\",data.shape)\n",
    "\n",
    "def get_fraud_labels(user_emails):\n",
    "    '''Remove whitelisted or test emails'''\n",
    "\n",
    "    # get all the events related to the requests aka within 60 minutes before the first request for the users who mader requests\n",
    "    bl_emails = [r['email'] for r in list(client['production']['emailBlacklistCollection'].find({'level': 'BLOCKED'}))]\n",
    " \n",
    "    return np.array([1 if user in bl_emails else 0 for user in user_emails])\n",
    "\n",
    "fraud = get_fraud_labels(user_emails)\n",
    "\n",
    "# set up data\n",
    "X = data\n",
    "y = fraud\n",
    "groups = np.array(user_emails)\n",
    "\n",
    "# create a dict with the results\n",
    "results_dict = {'X': X,\n",
    "                'y': y, \n",
    "                'groups': groups, \n",
    "                'feature_names': np.array(data_df.columns)}\n",
    "\n",
    "print(\"Saving results to a pickle file.\")\n",
    "\n",
    "# save results to a pickle file\n",
    "with open(\"../lstm_data_prep_pipeline/results/all_request_results.pickle\", 'wb') as outfile:\n",
    "    pickle.dump(results_dict, outfile, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
