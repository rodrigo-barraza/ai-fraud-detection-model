{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for numerical work\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pymongo\n",
    "\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "from pandas.io.json import json_normalize\n",
    "from pymongo import MongoClient\n",
    "\n",
    "import pickle\n",
    "\n",
    "# load the database credentials from file\n",
    "with open('../creds/creds.json') as json_data:\n",
    "    creds = json.load(json_data)\n",
    "    \n",
    "client = MongoClient(creds['connection_string'])\n",
    "    \n",
    "# get the new credit card and interac requests\n",
    "all_requests = list(client['ml']['requestEvents60'].find())\n",
    "\n",
    "# flatten into one object per event\n",
    "flat_requests = [{'request': rs['request'], \n",
    "                  'event': event} for rs in all_requests for event in rs['events']]\n",
    "\n",
    "# flatten requests into a dataframe\n",
    "all_events = pd.DataFrame(json_normalize(flat_requests))\n",
    "\n",
    "# create a dataframe with the results\n",
    "df_with_id = all_events\n",
    "\n",
    "# sort by request id and date\n",
    "df_with_id = df_with_id.sort_values(by=['request._id','event.created'])\n",
    "\n",
    "# calculate the previous event time and the time between events\n",
    "df_with_id['previous_event_time'] = df_with_id.groupby(['request._id'])['event.created'].shift(1)\n",
    "df_with_id['event.time_since_last_event'] = pd.to_numeric(df_with_id['event.created']-df_with_id['previous_event_time'])*1e-9\n",
    "\n",
    "# replace string versions of infinity with proper inf object\n",
    "df_with_id = df_with_id.replace('Infinity', np.inf)\n",
    "\n",
    "# convert columns that should be to numeric\n",
    "df_with_id['request.metadata.amount'] = pd.to_numeric(df_with_id['request.metadata.amount'])\n",
    "df_with_id['request.metadata.rate'] = pd.to_numeric(df_with_id['request.metadata.rate'])\n",
    "df_with_id['request.metadata.cents'] = pd.to_numeric(df_with_id['request.metadata.cents'])\n",
    "df_with_id['request.value'] = pd.to_numeric(df_with_id['request.value'])\n",
    "df_with_id['event.metadata.amount'] = pd.to_numeric(df_with_id['event.metadata.amount'])\n",
    "df_with_id['event.metadata.rate'] = pd.to_numeric(df_with_id['event.metadata.rate'])\n",
    "df_with_id['event.metadata.cents'] = pd.to_numeric(df_with_id['event.metadata.cents'])\n",
    "df_with_id['event.value'] = pd.to_numeric(df_with_id['event.value'])\n",
    "\n",
    "# get the days since november\n",
    "df_with_id['event.days_since_nov'] = df_with_id['event.created'].apply(lambda x: (x - datetime.datetime(year=2017,month=11,day=1)).days)\n",
    "\n",
    "# replace older bitcoin labels with new format\n",
    "df_with_id.loc[df_with_id['event.eventLabel'].str.lower() == 'bitcoin', 'event.eventLabel'] = 'BTC'\n",
    "\n",
    "# create unique category identifiers\n",
    "df_with_id['event.category_action_label'] = df_with_id['event.eventCategory']+'_'+df_with_id['event.eventAction']+'_'+df_with_id['event.eventLabel']\n",
    "df_with_id['event.category_action'] = df_with_id['event.eventCategory']+'_'+df_with_id['event.eventAction']\n",
    "\n",
    "# drop columns that contain list/array values because they can't be processed\n",
    "list_drops = [col for col in df_with_id.columns if df_with_id[col].apply(lambda x: type(x)).value_counts().index[0] == \"<class 'list'>\"]\n",
    "df_with_id = df_with_id.drop(list_drops, axis=1)\n",
    "\n",
    "# drop some other columns\n",
    "df_with_id = df_with_id.drop(['event.metadata.authResponseEIN.body.data.token_type','event.metadata.authResponseEIN.headers.map.content-type'], axis=1)\n",
    "\n",
    "# categorical columns that need to be converted to binary\n",
    "categorical_columns = ['event.category_action',\n",
    "                        'event.category_action_label',\n",
    "                        'event.metadata.addressCity',\n",
    "                        'event.metadata.addressCountry',\n",
    "                        'event.metadata.addressProvince',\n",
    "                        'event.metadata.city',\n",
    "                        'event.metadata.country',\n",
    "                        'event.metadata.currency',\n",
    "                        'event.metadata.instrument',\n",
    "                        'event.metadata.mongoResponse.product',\n",
    "                        'event.metadata.product',\n",
    "                        'event.metadata.productId',\n",
    "                        'event.metadata.prossessorError.billingDetails.city',\n",
    "                        'event.metadata.prossessorError.billingDetails.country',\n",
    "                        'event.metadata.prossessorError.billingDetails.state',\n",
    "                        'event.metadata.prossessorError.card.type',\n",
    "                        'event.metadata.prossessorError.currencyCode',\n",
    "                        'event.metadata.prossessorResponse.billingDetails.city',\n",
    "                        'event.metadata.prossessorResponse.billingDetails.country',\n",
    "                        'event.metadata.prossessorResponse.billingDetails.province',\n",
    "                        'event.metadata.prossessorResponse.billingDetails.state',\n",
    "                        'event.metadata.prossessorResponse.card.cardType',\n",
    "                        'event.metadata.prossessorResponse.card.type',\n",
    "                        'event.metadata.prossessorResponse.card_type',\n",
    "                        'event.metadata.prossessorResponse.currency',\n",
    "                        'event.metadata.prossessorResponse.currencyCode',\n",
    "                        'event.metadata.province',\n",
    "                        'event.metadata.requestParams.currency',\n",
    "                        'event.metadata.requestParams.product',\n",
    "                        'event.metadata.type']\n",
    "\n",
    "\n",
    "unique_columns = ['event.metadata.bankName',\n",
    "                 'event.metadata.cardHolder',\n",
    "                 'event.metadata.cardId',\n",
    "                 'event.metadata.cardName',\n",
    "                 'event.metadata.cardNumberLastFour',\n",
    "                 'event.metadata.cardPrefix',\n",
    "                 'event.metadata.cardSuffix',\n",
    "                 'event.metadata.email',\n",
    "                 'event.metadata.firstName',\n",
    "                 'event.metadata.fullName',\n",
    "                 'event.metadata.lastName',\n",
    "                 'event.metadata.mongoResponse.email',\n",
    "                 'event.metadata.name',\n",
    "                 'event.metadata.prossessorError.card.cardExpiry.month',\n",
    "                 'event.metadata.prossessorError.card.cardExpiry.year',\n",
    "                 'event.metadata.prossessorError.card.lastDigits',\n",
    "                 'event.metadata.prossessorError.card.type',\n",
    "                 'event.metadata.prossessorResponse.card.cardExpiry.month',\n",
    "                 'event.metadata.prossessorResponse.card.cardExpiry.year',\n",
    "                 'event.metadata.prossessorResponse.card.cardType',\n",
    "                 'event.metadata.prossessorResponse.card.lastDigits',\n",
    "                 'event.metadata.prossessorResponse.card.type',\n",
    "                 'event.metadata.prossessorResponse.card_expiry_month',\n",
    "                 'event.metadata.prossessorResponse.card_expiry_year',\n",
    "                 'event.metadata.prossessorResponse.card_suffix',\n",
    "                 'event.metadata.prossessorResponse.card_type',\n",
    "                 'event.metadata.prossessorResponse.profile.email',\n",
    "                 'event.metadata.prossessorResponse.profile.firstName',\n",
    "                 'event.metadata.prossessorResponse.profile.lastName',\n",
    "                 'event.metadata.requestParams.card_id',\n",
    "                 'event.metadata.requestParams.email']\n",
    "\n",
    "numerical_per_currency = ['event.metadata.amount',\n",
    "                         'event.metadata.blockioResponse.data.amount_sent',\n",
    "                         'event.metadata.blockioResponse.data.amount_withdrawn',\n",
    "                         'event.metadata.lastTradedPx',\n",
    "                         'event.metadata.mongoResponse.amount',\n",
    "                         'event.metadata.mongoResponse.price',\n",
    "                         'event.metadata.price',\n",
    "                         'event.metadata.prossessorResponse.amount',\n",
    "                         'event.metadata.rate',\n",
    "                         'event.metadata.requestParams.amount',\n",
    "                         'event.metadata.requestParams.price',\n",
    "                         'event.metadata.requestParams.product_amount']\n",
    "\n",
    "numerical_overall = ['event.metadata.cents',\n",
    "                     'event.metadata.prossessorResponse.charge_amount',\n",
    "                     'event.metadata.requestParams.charge_amount',\n",
    "                     'event.value',\n",
    "                     'event.time_since_last_event',\n",
    "                     'event.days_since_nov']\n",
    "\n",
    "all_columns = list(set(categorical_columns + numerical_per_currency + numerical_overall + unique_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert columns to either numeric or categorical\n",
    "def convert_to_numeric_or_lower_str(column):\n",
    "    \n",
    "        try:\n",
    "            return pd.to_numeric(col)\n",
    "\n",
    "        except:\n",
    "            return col.str.lower()\n",
    "\n",
    "df_with_id = df_with_id.apply(convert_to_numeric_or_lower_str, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize the columns where unique values matter\n",
    "unique_data = df_with_id[['request._id']+unique_columns]\n",
    "\n",
    "def n_unique(series):\n",
    "    \n",
    "    return series.dropna().unique().size\n",
    "\n",
    "def n_NaN(series):\n",
    "    \n",
    "    return np.sum(series.isnull())\n",
    "\n",
    "unique_summary = unique_data.groupby(['request._id'])[unique_columns].agg([n_unique, n_NaN])\n",
    "unique_summary.columns = [col[0] if col[1] == '' else col[0]+'_'+col[1] for col in unique_summary.columns.ravel()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize numerical by currency\n",
    "numerical_by_currency_data = df_with_id[['request._id','event.category_action_label']+numerical_per_currency]\n",
    "numerical_by_currency_data.dropna(axis=0, how='all', subset=numerical_per_currency, inplace=True)\n",
    "groupby_agg = numerical_by_currency_data.groupby(['request._id','event.category_action_label'], as_index=False)[numerical_per_currency].agg(['mean','median','max','min','std'])\n",
    "groupby_agg.columns = [col[0] if col[1] == '' else col[0]+'_'+col[1] for col in groupby_agg.columns.ravel()]\n",
    "groupby_agg = groupby_agg.reset_index()\n",
    "groupby_agg = groupby_agg.melt(id_vars=['request._id','event.category_action_label'])\n",
    "groupby_agg['variable'] = groupby_agg['event.category_action_label']+'_'+groupby_agg['variable']\n",
    "groupby_agg.drop('event.category_action_label', axis=1, inplace=True)\n",
    "groupby_agg = groupby_agg.pivot(index='request._id', columns='variable', values='value').reset_index().set_index('request._id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_with_id[all_columns]\n",
    "\n",
    "df = convert_to_numeric_or_lower_str(df)\n",
    "\n",
    "same_cols = generate_same_columns_df(df)\n",
    "\n",
    "filled_df = fill_same_values(df, same_cols)\n",
    "\n",
    "filled_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[col for col in sorted(df.columns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_level_cols = [col for col in same_cols.col.unique() if col.count('.') < 3]\n",
    "drop_cols = []\n",
    "levels = np.max([col.count('.') for col in same_cols.col.unique()])\n",
    "\n",
    "for i in range(levels):\n",
    "    for col in [col for col in same_cols.col.unique() if col.count('.') == i]:\n",
    "\n",
    "        drop_cols += list(same_cols[(same_cols.col == col) & (same_cols.same_percent_of_other > 0.95)].other_col.unique())\n",
    "\n",
    "drop_cols = list(set(drop_cols))\n",
    "\n",
    "sorted(clean_df.columns.drop(drop_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def convert(name):\n",
    "    s1 = re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', name)\n",
    "    return re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', s1).lower().replace('.','_')\n",
    "\n",
    "def combine_same_name_columns(df):\n",
    "    \n",
    "    clean_df = df\n",
    "    \n",
    "    columns = df.columns\n",
    "    \n",
    "    drops = []\n",
    "    \n",
    "    for col in columns:\n",
    "        \n",
    "        if col not in drops:\n",
    "            other_cols = columns.drop(col)\n",
    "            for other_col in other_cols:\n",
    "                if other_col not in drops:\n",
    "                    if convert(col) == convert(other_col):\n",
    "                        print(col,'is same as',other_col, 'filling values and dropping', other_col)\n",
    "                        clean_df.loc[clean_df[col].isnull(), col] = clean_df[clean_df.isnull()][other_col]\n",
    "                        clean_df = clean_df.drop(other_col, axis=1)\n",
    "                        drops.append(other_col)\n",
    "                \n",
    "    \n",
    "    return clean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaner_df = combine_same_name_columns(clean_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the columns that are the same as other columns\n",
    "def same_column_mappings(df, min_similarity=0.95):\n",
    "\n",
    "    results = []\n",
    "    \n",
    "    processed = []\n",
    "    \n",
    "    # convert columns to numeric if possible\n",
    "    def try_numeric(col):\n",
    "        try:\n",
    "            return pd.to_numeric(col)\n",
    "        except:\n",
    "            return col\n",
    "    \n",
    "    df = df.apply(try_numeric, axis=1)\n",
    "    \n",
    "    df = df.replace('Infinity', np.nan)\n",
    "\n",
    "    # get the columns sorted by number of actual values present\n",
    "    columns = pd.DataFrame(df.count()).reset_index().sort_values(by=0, ascending=False)['index']\n",
    "    \n",
    "    # do the same for each type of column\n",
    "    numerical = df.select_dtypes(include=['float64','int64'])\n",
    "    non_numerical = df.select_dtypes(include='object')\n",
    "    \n",
    "    numerical_cols = pd.DataFrame(numerical.count()).reset_index().sort_values(by=0, ascending=False)['index']\n",
    "    non_numerical_cols = pd.DataFrame(non_numerical.count()).reset_index().sort_values(by=0, ascending=False)['index']\n",
    "\n",
    "    # for each column \n",
    "    for col in columns:\n",
    "        \n",
    "        # if it hasn't been processed yet\n",
    "        if col not in processed:\n",
    "            result = {'column': col, 'same_cols': []}\n",
    "            processed.append(col)\n",
    "            print(\"Processing: \", col)\n",
    "\n",
    "            if df[col].dtype == 'float64' or df[col].dtype == 'int64':\n",
    "\n",
    "                for other_col in numerical_cols[numerical_cols != col].values:\n",
    "                    if other_col not in processed:\n",
    "                        sub_df = df[[col,other_col]].dropna(how='any')\n",
    "\n",
    "                        try:\n",
    "                            same = np.sum(pd.to_numeric(sub_df[col]) == pd.to_numeric(sub_df[other_col]))\n",
    "                            same_percentage = same/sub_df[other_col].dropna().size\n",
    "\n",
    "                            if same_percentage >= min_similarity:\n",
    "\n",
    "                                result['same_cols'].append(other_col)\n",
    "                                processed.append(other_col)\n",
    "\n",
    "                        except Exception as e:\n",
    "                            print(e)\n",
    "\n",
    "            else:\n",
    "\n",
    "                for other_col in non_numerical_cols[non_numerical_cols != col].values:\n",
    "                    if other_col not in processed:\n",
    "                        sub_df = df[[col,other_col]].dropna(how='any')\n",
    "\n",
    "                        try:\n",
    "                            same = np.sum(sub_df[col] == sub_df[other_col])\n",
    "                            same_percentage = same/sub_df[other_col].dropna().size\n",
    "\n",
    "                            if same_percentage >= min_similarity:\n",
    "\n",
    "                                result['same_cols'].append(other_col)\n",
    "                                processed.append(other_col)\n",
    "\n",
    "                        except Exception as e:\n",
    "                            print(e)\n",
    "\n",
    "            if len(result['same_cols']) > 0: \n",
    "                results.append(result)\n",
    "\n",
    "    return results\n",
    "\n",
    "same_mappings = same_column_mappings(df_with_id[all_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "same_mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_same_cols(df, same_mappings):\n",
    "    \n",
    "    for col_set in same_mappings:\n",
    "        \n",
    "        column = col_set['column']\n",
    "        other_cols = col_set['same_cols']\n",
    "        \n",
    "        for other_col in other_cols:\n",
    "            \n",
    "            df.loc[df[column].isnull(), column] = df[df[column].isnull()][other_col].values\n",
    "            df = df.drop(other_col, axis=1)\n",
    "            print(\"Dropped\", other_col)\n",
    "            \n",
    "    return df\n",
    "\n",
    "\n",
    "clean_df = combine_same_cols(df_with_id[all_columns], same_mappings)\n",
    "\n",
    "[col for col in sorted(clean_df.columns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at places where the email is different from the processor email\n",
    "ev = df[df['event.metadata.email'] != df['event.metadata.prossessorResponse.profile.email']][['event.category_action_label','event.metadata.email','event.metadata.prossessorResponse.profile.email']].dropna()\n",
    "ev = ev.drop('event.category_action_label', axis=1)\n",
    "\n",
    "ev.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort ascending by request_id and descending by time\n",
    "subset = subset.sort_values(by=['request_id','created'], ascending=[True, False])\n",
    "\n",
    "# grab the emails\n",
    "user_emails = subset['request_email']\n",
    "\n",
    "# get the fraudulent emails\n",
    "def get_fraud_labels(user_emails):\n",
    "    '''Remove whitelisted or test emails'''\n",
    "\n",
    "    # get all the events related to the requests aka within 60 minutes before the first request for the users who mader requests\n",
    "    bl_emails = [r['email'] for r in list(client['production']['emailBlacklistCollection'].find({'level': 'BLOCKED'}))]\n",
    " \n",
    "    return np.array([1 if user in bl_emails else 0 for user in user_emails])\n",
    "\n",
    "# get the fraud labels\n",
    "fraud = get_fraud_labels(user_emails)\n",
    "\n",
    "# drop the time and email columns\n",
    "subset = subset.drop(['request_email','created'], axis = 1)\n",
    "subset = subset.reset_index(drop=True)\n",
    "\n",
    "# hacky way to number the sequence events prior to each interac request with 0 being the request and 10 being the 10th event prior to the request\n",
    "subset['index_int'] = subset.index\n",
    "event_index = subset.groupby('request_id')['index_int'].agg(lambda x: list(np.abs(min(x)-x)))\n",
    "all_index = [[item] if type(item) == type(int) else list(item) for item in event_index]\n",
    "all_index = [item for sublist in all_index for item in sublist]\n",
    "subset['timesteps'] = all_index\n",
    "\n",
    "print(\"Dataframe is\",np.sum(subset.memory_usage())*1e-9,'gigabytes in memory')\n",
    "\n",
    "print(\"Saving to hdf5 file\")\n",
    "\n",
    "subset.to_hdf('../lstm_data_prep_pipeline/results/all_request_events.hdf5', 'table')\n",
    "\n",
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
