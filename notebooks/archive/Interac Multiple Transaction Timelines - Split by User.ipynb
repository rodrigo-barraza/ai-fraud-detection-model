{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import juno.junoutils as junoutils\n",
    "import juno.junodb as junodb\n",
    "import json\n",
    "import datetime\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in the previously summarized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the previously summarized data\n",
    "results_df = pd.read_csv('../ten_minute_pipeline/data/interac_transaction_summary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the database credentials from file\n",
    "with open('../user_aggregation_pipeline//creds.json') as json_data:\n",
    "    creds = json.load(json_data)\n",
    "\n",
    "# set up a database with credentials\n",
    "db = junodb.Database(creds)\n",
    "\n",
    "# get the full history of interac requests\n",
    "all_interac_requests = list(db._client['production']['eventCollection'].find({\n",
    "        'eventCategory': 'interac',\n",
    "        'eventAction': 'request',\n",
    "        'metadata.email': {'$ne': None}}))\n",
    "\n",
    "# flatten objects into a pandas dataframe\n",
    "interac = junoutils.flattenObjects(all_interac_requests)\n",
    "\n",
    "# subset the columns\n",
    "cols_to_use = ['created','eventLabel','metadata.amount','metadata.email','metadata.rate','value']\n",
    "\n",
    "interac = interac[cols_to_use]\n",
    "\n",
    "\n",
    "interac.columns = ['created','currency','amount','email','rate','value']\n",
    "\n",
    "interac = pd.get_dummies(data=interac, columns=['currency'])\n",
    "\n",
    "interac = interac.sort_values(by='created')\n",
    "\n",
    "interac['created'] = pd.to_datetime(interac.created)\n",
    "interac['amount'] = pd.to_numeric(interac.amount)\n",
    "interac['rate'] = pd.to_numeric(interac.rate)\n",
    "interac['value'] = pd.to_numeric(interac.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPriorUserEvents(email, request_time=None, lookback_s=None):\n",
    "    \n",
    "    if (lookback_s == None) & (request_time != None):\n",
    "        # find all events for the user before the request_time and after or at 30 days ago\n",
    "        events_json = list(db._client['production']['eventCollection'].find({\n",
    "                'metadata.email': email,\n",
    "                'created': {'$lt': request_time}\n",
    "        }))\n",
    "    \n",
    "    elif (lookback_s != None) & (request_time == None):\n",
    "        \n",
    "        lookback_time = datetime.datetime.now() - datetime.timedelta(seconds=lookback_s)\n",
    "        \n",
    "        # find all events for the user before the request_time and after or at 30 days ago\n",
    "        events_json = list(db._client['production']['eventCollection'].find({\n",
    "                'metadata.email': email,\n",
    "                'created': {'$gte': lookback_time}\n",
    "        }))\n",
    "                           \n",
    "    elif (lookback_s != None) & (request_time != None):\n",
    "        \n",
    "        lookback_time = request_time - datetime.timedelta(seconds=lookback_s)\n",
    "        \n",
    "        # find all events for the user before the request_time and after or at 30 days ago\n",
    "        events_json = list(db._client['production']['eventCollection'].find({\n",
    "                'metadata.email': email,\n",
    "                'created': {'$lt': request_time, '$gte': lookback_time}\n",
    "        }))\n",
    "                           \n",
    "    else:\n",
    "        # find all events for the user before the request_time and after or at 30 days ago\n",
    "        events_json = list(db._client['production']['eventCollection'].find({\n",
    "                'metadata.email': email\n",
    "        }))\n",
    "    \n",
    "    # flatten JSON objects into a pandas dataframe\n",
    "    request_df = junoutils.flattenObjects(events_json)\n",
    "    \n",
    "    return request_df\n",
    "\n",
    "def summarizeTransactionActions(df):\n",
    "    \n",
    "    record = {}\n",
    "    record['n'] = df.shape[0]\n",
    "    record['mean_value'] = df.value.mean()\n",
    "    record['mean_timebetween'] = df.time_since_last.mean()\n",
    "    \n",
    "    # for each label column\n",
    "    for col in [col for col in df.columns if 'eventLabel_' in col]:\n",
    "        record['n_'+col] = np.sum(df[col])\n",
    "        \n",
    "    return record\n",
    "\n",
    "def summarizeInteracActivity(df):\n",
    "    \n",
    "    if df.shape[0] == 0:\n",
    "        return {}\n",
    "    \n",
    "    # subset the events to just the transaction related events\n",
    "    this_df = df[(df.eventCategory == 'interac')]# & (df.eventAction.isin(['request','rejected','fulfilled']))]\n",
    "    \n",
    "    if this_df.shape[0] == 0:\n",
    "        return {}\n",
    "    \n",
    "    # only use columns related to transactions\n",
    "    # this_df = this_df[['created','eventCategory','eventAction','eventLabel', 'metadata.amount','metadata.rate','value']]\n",
    "    \n",
    "    # combine the eventCategory and eventAction into a unique key\n",
    "    this_df['ca'] = this_df.eventCategory + '_' + this_df.eventAction\n",
    "    \n",
    "    # get unique interac combinations\n",
    "    transaction_actions = this_df.ca.unique()\n",
    "    \n",
    "    record = {}\n",
    "    \n",
    "    # separate out dataframes\n",
    "    for category_action in transaction_actions:\n",
    "        \n",
    "        ca_df = this_df[this_df.ca == category_action]\n",
    "        ca_df = ca_df.sort_values(by='created')\n",
    "        ca_df['previous_request'] = ca_df['created'].shift(1)\n",
    "        ca_df = ca_df.dropna()\n",
    "        ca_df['time_since_last'] = (ca_df['created'] - ca_df['previous_request']).astype(int)*1e-9 # convert time since last event to seconds\n",
    "        ca_df = pd.get_dummies(ca_df, columns=['eventLabel'])\n",
    "        \n",
    "        record[category_action] = summarizeTransactionActions(ca_df)\n",
    "        \n",
    "    return record\n",
    "    \n",
    "def summarizeEvents(df):\n",
    "    \n",
    "    if df.shape[0] == 0:\n",
    "        return {}\n",
    "    \n",
    "    # get rid of session ids from eventLabel\n",
    "    this_df = df[(df.eventAction != 'start') & (df.eventCategory != 'session')]\n",
    "    \n",
    "    if this_df.shape[0] == 0:\n",
    "        return {}\n",
    "    \n",
    "    this_df = this_df[['created','eventCategory','eventAction','eventLabel']]\n",
    "    \n",
    "    # combine the eventCategory and eventAction into a unique key\n",
    "    this_df['ca'] = this_df.eventCategory + '_' + this_df.eventAction\n",
    "    this_df['cal'] = this_df.eventCategory + '_' + this_df.eventAction + '_' + this_df.eventLabel\n",
    "    this_df = this_df.sort_values(by='created')\n",
    "    this_df['previous_request'] = this_df['created'].shift(1)\n",
    "    this_df['previous_ca'] = this_df['ca'].shift(1)\n",
    "    this_df['previous_cal'] = this_df['cal'].shift(1)\n",
    "    this_df = this_df.dropna()\n",
    "    \n",
    "    # need to figure out how to weed out times that are obviously a long time ago doesn't make sense to keep those\n",
    "    this_df['time_since_last'] = (this_df['created'] - this_df['previous_request']).astype(int)*1e-9 # convert time since last event to seconds\n",
    "    this_df['st_ca'] = this_df['previous_ca'] + '_' + this_df['ca']\n",
    "    this_df['st_cal'] = this_df['previous_cal'] + '_' + this_df['cal']\n",
    "    this_df = pd.get_dummies(this_df, columns=['ca','st_ca','st_cal'])\n",
    "    \n",
    "    record = {}\n",
    "    \n",
    "    record['mean_timesince'] = this_df.time_since_last.mean()\n",
    "    \n",
    "    # for each state or transition column\n",
    "    for col in [col for col in this_df.columns if 'ca_' in col] + [col for col in this_df.columns if 'cal_' in col]:\n",
    "        record['n_'+col] = np.sum(this_df[col])\n",
    "        \n",
    "    return record\n",
    "\n",
    "def summarizePreEventsMultiple(email, request_time=datetime.datetime.now(), lookback_s_list=None):\n",
    "\n",
    "    result = {}\n",
    "    \n",
    "    if lookback_s_list != None:\n",
    "        # sort the list of seconds into descending order to limit requests\n",
    "        sorted_seconds = sorted(lookback_s_list, reverse=True)\n",
    "        \n",
    "        this_df = getPriorUserEvents(email=email, request_time=request_time, lookback_s=sorted_seconds[0])\n",
    "        \n",
    "        if this_df.shape[0] == 0:\n",
    "            return {}\n",
    "        \n",
    "        col_name = 'mins_' + str(int(sorted_seconds[0]/60))\n",
    "        \n",
    "        result[col_name] = {**summarizeInteracActivity(this_df), **summarizeEvents(this_df)} \n",
    "        \n",
    "        # for the shorter time periods\n",
    "        for seconds in sorted_seconds[1:]:\n",
    "            \n",
    "            this_lookback = request_time - datetime.timedelta(seconds=seconds)\n",
    "            \n",
    "            this_df = this_df[this_df.created >= this_lookback]\n",
    "            \n",
    "            col_name = 'mins_' + str(int(seconds/60))\n",
    "        \n",
    "            result[col_name] = {**summarizeInteracActivity(this_df), **summarizeEvents(this_df)}\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        this_df = getPriorUserEvents(email=email, request_time=request_time)\n",
    "        result['now'] = {**summarizeInteracActivity(this_df), **summarizeEvents(this_df)}\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the new interac transactions since the last time the summary routine was run\n",
    "new_interac = interac[interac.created > results_df['request.created'].max()]\n",
    "new_interac.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize All of the New Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "ten_minutes_in_s = 60*10\n",
    "hour_in_s = 60*60\n",
    "day_in_s = 60*60*24\n",
    "week_in_s = 60*60*24*7\n",
    "days30_in_s = 60*60*24*30\n",
    "\n",
    "new_results = []\n",
    "fetch_times = []\n",
    "\n",
    "times = [ten_minutes_in_s, hour_in_s, day_in_s, week_in_s, days30_in_s]\n",
    "\n",
    "for i in np.arange(new_interac.shape[0]):\n",
    "    \n",
    "    now = time.time()\n",
    "    \n",
    "    result = {}\n",
    "    result['request'] = new_interac.loc[i,:].to_dict()\n",
    "    result['prior'] = summarizePreEventsMultiple(email=new_interac.email.values[i], request_time=new_interac.created[i], lookback_s_list=times)\n",
    "    new_results.append(result)\n",
    "    \n",
    "    end = time.time()\n",
    "    fetch_times.append(end-now)\n",
    "    print(i, 'of',new_interac.shape[0], 'Duration: ', end-now, 'Mean Time:', np.mean(fetch_times))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine the new and old records together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = new_results+results_df.to_dict(orient='records')\n",
    "all_results_df = junoutils.flattenObjects(all_results).fillna(0)\n",
    "all_results_df.loc[all_results_df.fraud == 0,'fraud'] = False\n",
    "\n",
    "fraudsters = ['gaelkevin@hotmail.com', 'royer.8383@gmail.com','adventurous7381@gmail.com']\n",
    "results_df['fraud'] = results_df['request.email'].isin(fraudsters)\n",
    "\n",
    "def RemoveWhitelistRecords(df):\n",
    "    \n",
    "    db = junodb.Database(creds)\n",
    "\n",
    "    wlemails = junoutils.flattenObjects(list(db._client['production']['emailWhitelistCollection'].find({'level': 'ALLOWED'}))).email\n",
    "    \n",
    "    df = df[df['request.email'].str.contains('test') == False]\n",
    "    df = df[df['request.email'].str.contains('fingerfoodstudios') == False]\n",
    "    df = df[df['request.email'].isin(wlemails) == False]\n",
    "    \n",
    "    return df \n",
    "\n",
    "records_clean = RemoveWhitelistRecords(all_results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the new dataset back to csv\n",
    "records_clean.to_csv('../ten_minute_pipeline/data/interac_transaction_summary.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the dataset which is unbalanced into randomized and stratified training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_users_rand(df):   \n",
    "    X = df.drop(['request.created'], axis = 1)\n",
    "    \n",
    "    # define the split percentage for test\n",
    "    split_percentage = 1/3\n",
    "    \n",
    "    # get the fraudulent users and normal users\n",
    "    fraud_users = df['request.email'][df.fraud == True].unique()\n",
    "    normal_users = df['request.email'][df.fraud == False].unique()\n",
    "    \n",
    "    # get the number of each class\n",
    "    n_fraud_users = len(fraud_users)\n",
    "    n_normal_users = len(normal_users)\n",
    "    \n",
    "    # randomly shuffle each class\n",
    "    rand_fraud_users_idx = np.random.choice(a=np.arange(n_fraud_users), size=n_fraud_users, replace=False)\n",
    "    rand_normal_users_idx = np.random.choice(a=np.arange(n_normal_users), size=n_normal_users, replace=False)\n",
    "    \n",
    "    # get the number of test values for each class\n",
    "    n_fraud_users_test = int(split_percentage*n_fraud_users)\n",
    "    n_normal_users_test = int(split_percentage*n_normal_users)\n",
    "    \n",
    "    # randomly generate train and test user sets\n",
    "    rand_test_fraud_users = fraud_users[rand_fraud_users_idx[0:n_fraud_users_test]]\n",
    "    rand_test_normal_users = normal_users[rand_normal_users_idx[0:n_normal_users_test]]\n",
    "    rand_train_fraud_users = fraud_users[rand_fraud_users_idx[n_fraud_users_test:]]\n",
    "    rand_train_normal_users = normal_users[rand_normal_users_idx[n_normal_users_test:]]\n",
    "    \n",
    "    # manually split so there's no contamination between fraudulent and normal users\n",
    "    test_fraud = X[X['request.email'].isin(rand_test_fraud_users)].drop(['request.email'], axis=1)\n",
    "    train_fraud = X[X['request.email'].isin(rand_train_fraud_users)].drop(['request.email'], axis=1)\n",
    "    test_normal = X[X['request.email'].isin(rand_test_normal_users)].drop(['request.email'], axis=1)\n",
    "    train_normal = X[X['request.email'].isin(rand_train_normal_users)].drop(['request.email'], axis=1)\n",
    "\n",
    "    # get combine test and train together\n",
    "    test = pd.concat((test_fraud,test_normal))\n",
    "    train = pd.concat((train_fraud,train_normal))\n",
    "\n",
    "    # split into labels and data\n",
    "    X_test = test.drop('fraud', axis = 1)\n",
    "    X_train = train.drop('fraud', axis = 1)\n",
    "    y_test = test['fraud']\n",
    "    y_train = train['fraud']\n",
    "    \n",
    "    return (X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = split_by_users_rand(records_clean)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigate Different Time Ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaveOneUserOutCrossValidation(df, classifier_list):\n",
    "    \n",
    "    # list of different lookback times\n",
    "    times = [['10'],['60'],['1440'],['10080'],['43200'],['10','60'], ['10','60','1440']]\n",
    "\n",
    "    # variables to keep track of\n",
    "    times_records = []\n",
    "    user_as_test = []\n",
    "    training_fraud_percentages = []\n",
    "    testing_fraud_percentages = []\n",
    "    training_accuracies = []\n",
    "    testing_accuracies = []\n",
    "    training_average_precisions = []\n",
    "    testing_average_precisions = []\n",
    "    train_true_negatives = []\n",
    "    train_false_negatives = []\n",
    "    train_true_positives = []\n",
    "    train_false_positives = []\n",
    "    test_true_negatives = []\n",
    "    test_false_negatives = []\n",
    "    test_true_positives = []\n",
    "    test_false_positives = []\n",
    "    classifiers = []\n",
    "    \n",
    "    # for each classifer type\n",
    "    for clf in classifier_list:\n",
    "        \n",
    "        classifier = clf['Label']\n",
    "        model = clf['Model']\n",
    "\n",
    "        # for each time period or combination of time periods\n",
    "        for time in times:\n",
    "\n",
    "            prefixes = ['prior.mins_'+t+'.' for t in time]\n",
    "\n",
    "            columns = ['fraud','request.email','request.created']\n",
    "\n",
    "            # get only the columns for the currently seclected time periods\n",
    "            last_60_columns = list(set([col if any(prefix in col for prefix in prefixes) else None for col in results_df.columns])-set([None]))\n",
    "\n",
    "            # get the data for the last 10 columns\n",
    "            X = records_clean[last_60_columns].values\n",
    "            y = records_clean['fraud'].values\n",
    "            emails = records_clean['request.email']\n",
    "\n",
    "            group_kfold = GroupKFold(n_splits=3)\n",
    "            group_kfold.get_n_splits(X=X, y=y, groups=emails)\n",
    "\n",
    "            for train_index, test_index in group_kfold.split(X=X, y=y, groups=emails):\n",
    "\n",
    "                X_train = X[train_index,:]\n",
    "                X_test = X[test_index,:]\n",
    "                y_train = y[train_index]\n",
    "                y_test = y[test_index]\n",
    "                \n",
    "                class_ratio = np.sum(y_train == 0)/np.sum(y_train == 1)\n",
    "                \n",
    "                if type(model) == type(XGBClassifier()):\n",
    "                    model.set_params(**{'scale_pos_weight': class_ratio})\n",
    "\n",
    "                model.fit(X_train, y_train)\n",
    "\n",
    "                preds_train = model.predict(X_train)\n",
    "                preds_test = model.predict(X_test)\n",
    "                probs_train = model.predict_proba(X_train)[:,1]\n",
    "                probs_test = model.predict_proba(X_test)[:,1]\n",
    "\n",
    "                training_accuracy = np.sum(preds_train == y_train)/len(y_train)\n",
    "                testing_accuracy = np.sum(preds_test == y_test)/len(y_test)\n",
    "\n",
    "                training_avp = sklearn.metrics.average_precision_score(y_train, probs_train)\n",
    "                testing_avp = sklearn.metrics.average_precision_score(y_test, probs_test)\n",
    "\n",
    "                train_cm = sklearn.metrics.confusion_matrix(y_train, preds_train, sample_weight=None)\n",
    "\n",
    "                test_cm = sklearn.metrics.confusion_matrix(y_test, preds_test, sample_weight=None)\n",
    "\n",
    "                times_records.append(time)\n",
    "                training_fraud_percentages.append(np.sum(y_train)/len(y_train))\n",
    "                testing_fraud_percentages.append(np.sum(y_test)/len(y_test))\n",
    "                training_accuracies.append(training_accuracy)\n",
    "                testing_accuracies.append(testing_accuracy)\n",
    "                training_average_precisions.append(training_avp)\n",
    "                testing_average_precisions.append(testing_avp)\n",
    "                train_true_negatives.append(train_cm[0,0])\n",
    "                train_false_negatives.append(train_cm[1,0])\n",
    "                train_true_positives.append(train_cm[1,1])\n",
    "                train_false_positives.append(train_cm[0,1])\n",
    "                test_true_negatives.append(test_cm[0,0])\n",
    "                test_false_negatives.append(test_cm[1,0])\n",
    "                test_true_positives.append(test_cm[1,1])\n",
    "                test_false_positives.append(test_cm[0,1])\n",
    "                classifiers.append(classifier)\n",
    "\n",
    "    \n",
    "    # put results together in a dataframe\n",
    "    model_results_df = pd.DataFrame({\n",
    "        'classifier': classifiers,\n",
    "        'lookback': [time[0] if len(time) == 1 else '_'.join(time) for time in times_records],\n",
    "        'train_fraud_percentage': training_fraud_percentages,\n",
    "        'test_fraud_percentage': testing_fraud_percentages,\n",
    "        'train_accuracy': training_accuracies,\n",
    "        'test_accuracy': testing_accuracies,\n",
    "        'train_average_precision': training_average_precisions,\n",
    "        'test_average_precision': testing_average_precisions,\n",
    "        'train_true_negatives': train_true_negatives,\n",
    "        'train_false_negatives': train_false_negatives,\n",
    "        'train_true_positives': train_true_positives,\n",
    "        'train_false_positives': train_false_positives,\n",
    "        'test_true_negatives': test_true_negatives,\n",
    "        'test_false_negatives': test_false_negatives,\n",
    "        'test_true_positives': test_true_positives,\n",
    "        'test_false_positives': test_false_positives,\n",
    "    })     \n",
    "\n",
    "    # calculate precision and recall\n",
    "    model_results_df['test_precision'] = model_results_df.test_true_positives/(model_results_df.test_true_positives+model_results_df.test_false_positives)\n",
    "    model_results_df['test_recall'] = model_results_df.test_true_positives/(model_results_df.test_true_positives+model_results_df.test_false_negatives)\n",
    "    model_results_df['train_precision'] = model_results_df.train_true_positives/(model_results_df.train_true_positives+model_results_df.train_false_positives)\n",
    "    model_results_df['train_recall'] = model_results_df.train_true_positives/(model_results_df.train_true_positives+model_results_df.train_false_negatives)\n",
    "    model_results_df.fillna(0, inplace=True)\n",
    "\n",
    "    return model_results_df[sorted(model_results_df.columns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "model_results = None\n",
    "\n",
    "classifiers = [\n",
    "    {'Label': 'XGBoost', 'Model': XGBClassifier()},\n",
    "    {'Label': 'Modified XGBoost', 'Model': XGBClassifier(max_depth=9, subsample=0.92)},\n",
    "    {'Label': 'Random Forest', 'Model': RandomForestClassifier()},\n",
    "    {'Label': 'Modified Random Forest', 'Model': RandomForestClassifier(n_estimators=474, max_depth=15, max_features=int(X.shape[1]/100), class_weight='balanced')},\n",
    "    {'Label': 'Logistic Regression', 'Model': LogisticRegression(penalty='l1', class_weight='balanced')},\n",
    "    {'Label': 'Gradient Boosted Tree', 'Model': AdaBoostClassifier()},\n",
    "    {'Label': 'Multi Layer Perceptron', 'Model': MLPClassifier(alpha=0.0001, shuffle=True, hidden_layer_sizes=(100,3))},\n",
    "    {'Label': 'Dummy', 'Model': DummyClassifier()}\n",
    "]\n",
    "\n",
    "model_results = leaveOneUserOutCrossValidation(df=records_clean, classifier_list=classifiers)\n",
    "        \n",
    "summary = model_results.groupby(['classifier','lookback'], as_index=False)[['test_precision','test_recall','test_average_precision']].agg(['mean','median','std']).reset_index()\n",
    "summary.columns = [col[0] if col[1] == '' else '_'.join(col) for col in summary.columns.ravel()]\n",
    "summary.sort_values(by='test_average_precision_mean', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'prior.mins_60'\n",
    "\n",
    "X = records_clean\n",
    "X_all = records_clean[list(set([col if prefix in col else None for col in X.columns])-set([None]))]\n",
    "y_all = records_clean.fraud\n",
    "\n",
    "# get only the columns for the currently seclected time periods\n",
    "last_60_columns = ['request.email','request.created','fraud']+list(set([col if prefix in col else None for col in X.columns])-set([None]))\n",
    "\n",
    "mean_avps = []\n",
    "\n",
    "X_train, X_test, y_train, y_test = split_by_users_rand(X)\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=474, max_depth=15, max_features=959, class_weight='balanced')\n",
    "rf.fit(X_all,y_all)\n",
    " \n",
    "preds = rf.predict(X_all)\n",
    "probs = rf.predict_proba(X_all)\n",
    "\n",
    "print(sklearn.metrics.accuracy_score(y_all, preds))\n",
    "print(sklearn.metrics.confusion_matrix(y_all, preds))\n",
    "print(sklearn.metrics.average_precision_score(y_all, probs[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Hyperparameter Optimization Using Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt.space import Real, Integer\n",
    "from skopt.utils import use_named_args\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from skopt import gp_minimize\n",
    "\n",
    "max_possible_features = 1102\n",
    "\n",
    "# The list of hyper-parameters we want to optimize. For each one we define the bounds,\n",
    "# the corresponding scikit-learn parameter name, as well as how to sample values\n",
    "# from that dimension (`'log-uniform'` for the learning rate)\n",
    "space  = [Integer(1, 100, name='max_depth'),\n",
    "          Integer(1, max_possible_features, name='max_features'),\n",
    "          Integer(1, 1000, name='n_estimators')\n",
    "         ]\n",
    "\n",
    "# this decorator allows your objective function to receive a the parameters as\n",
    "# keyword arguments. This is particularly convenient when you want to set scikit-learn\n",
    "# estimator parameters\n",
    "@use_named_args(space)\n",
    "def mean_average_precision_of_RF(**params):\n",
    "    \n",
    "    \n",
    "    df = records_clean\n",
    "\n",
    "    prefixes = ['prior.mins_60']\n",
    "\n",
    "    columns = ['fraud','request.email','request.created']\n",
    "\n",
    "    # get only the columns for the currently seclected time periods\n",
    "    last_60_columns = ['fraud','request.email','request.created'] + list(set([col if any(prefix in col for prefix in prefixes) else None for col in results_df.columns])-set([None]))\n",
    "\n",
    "    # get the data for the last 10 columns\n",
    "    X = records_clean[last_60_columns]\n",
    "    \n",
    "    mavps = []\n",
    "    \n",
    "    for i in range(5):\n",
    "            \n",
    "        model = RandomForestClassifier(class_weight='balanced')\n",
    "\n",
    "        model.set_params(**params)\n",
    "\n",
    "        X_train, X_test, y_train, y_test = split_by_users_rand(X)\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        probs_test = model.predict_proba(X_test)[:,1]\n",
    "\n",
    "        testing_avp = sklearn.metrics.average_precision_score(y_test, probs_test)\n",
    "\n",
    "        mavps.append(testing_avp)\n",
    "\n",
    "    return -np.mean(mavps)\n",
    "                    \n",
    "res_gp = gp_minimize(mean_average_precision_of_RF, space, n_calls=50, random_state=0)\n",
    "\n",
    "print(\"Best score=%.4f\" % res_gp.fun)\n",
    "print(\"Hyperparameter Values\", res_gp.x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Hyperparameter Optimization Using Gradient Boosted Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from skopt.space import Real, Integer\n",
    "from skopt.utils import use_named_args\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from skopt import gp_minimize\n",
    "\n",
    "max_possible_features = 1102\n",
    "\n",
    "# The list of hyper-parameters we want to optimize. For each one we define the bounds,\n",
    "# the corresponding scikit-learn parameter name, as well as how to sample values\n",
    "# from that dimension (`'log-uniform'` for the learning rate)\n",
    "space  = [Real(0.001, 10, name='learning_rate'),\n",
    "          Integer(100, 1000, name='n_estimators')\n",
    "         ]\n",
    "\n",
    "# this decorator allows your objective function to receive a the parameters as\n",
    "# keyword arguments. This is particularly convenient when you want to set scikit-learn\n",
    "# estimator parameters\n",
    "@use_named_args(space)\n",
    "def mean_average_precision_of_RF(**params):\n",
    "    \n",
    "    df = records_clean\n",
    "    \n",
    "    tree = DecisionTreeClassifier(class_weight='balanced')\n",
    "    \n",
    "    model = AdaBoostClassifier(base_estimator=tree)\n",
    "                    \n",
    "    model.set_params(**params)\n",
    "\n",
    "    prefixes = ['prior.mins_60']\n",
    "\n",
    "    columns = ['fraud','request.email','request.created']\n",
    "\n",
    "    # get only the columns for the currently seclected time periods\n",
    "    last_60_columns = ['fraud','request.email','request.created'] + list(set([col if any(prefix in col for prefix in prefixes) else None for col in results_df.columns])-set([None]))\n",
    "\n",
    "    # get the data for the last 10 columns\n",
    "    X = records_clean[last_60_columns]\n",
    "    \n",
    "    mavps = []\n",
    "\n",
    "    # for each fraudulent user\n",
    "    for i in range(5):\n",
    "\n",
    "        X_train, X_test, y_train, y_test = split_by_users_rand(X)\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        probs_test = model.predict_proba(X_test)[:,1]\n",
    "\n",
    "        testing_avp = sklearn.metrics.average_precision_score(y_test, probs_test)\n",
    "\n",
    "        mavps.append(testing_avp)\n",
    "\n",
    "    return -np.mean(mavps)\n",
    "                    \n",
    "res_gp_adaboost = gp_minimize(mean_average_precision_of_RF, space, n_calls=50, random_state=0)\n",
    "\n",
    "print(res_gp_adaboost.x)\n",
    "print(res_gp_adaboost.fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Hyperparameter Optimization Using XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt.space import Real, Integer\n",
    "from skopt.utils import use_named_args\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from skopt import gp_minimize\n",
    "\n",
    "max_possible_features = 1102\n",
    "\n",
    "# The list of hyper-parameters we want to optimize. For each one we define the bounds,\n",
    "# the corresponding scikit-learn parameter name, as well as how to sample values\n",
    "# from that dimension (`'log-uniform'` for the learning rate)\n",
    "space  = [Integer(1, 10, name='max_depth'),\n",
    "          Real(0.5, 1, name='subsample')\n",
    "         ]\n",
    "\n",
    "# this decorator allows your objective function to receive a the parameters as\n",
    "# keyword arguments. This is particularly convenient when you want to set scikit-learn\n",
    "# estimator parameters\n",
    "@use_named_args(space)\n",
    "def mean_average_precision_of_RF(**params):\n",
    "    \n",
    "    df = records_clean\n",
    "\n",
    "    prefixes = ['prior.mins_60']\n",
    "\n",
    "    columns = ['fraud','request.email']\n",
    "\n",
    "    # get only the columns for the currently seclected time periods\n",
    "    last_60_columns = list(set([col if any(prefix in col for prefix in prefixes) else None for col in results_df.columns])-set([None]))\n",
    "\n",
    "    # get the data for the last 10 columns\n",
    "    X = records_clean[last_60_columns].values\n",
    "    y = records_clean['fraud'].values\n",
    "    emails = records_clean['request.email']\n",
    "\n",
    "    group_kfold = GroupKFold(n_splits=3)\n",
    "    group_kfold.get_n_splits(X=X, y=y, groups=emails)\n",
    "\n",
    "    mavps = []\n",
    "\n",
    "    for train_index, test_index in group_kfold.split(X=X, y=y, groups=emails):\n",
    "\n",
    "        X_train = X[train_index,:]\n",
    "        X_test = X[test_index,:]\n",
    "        y_train = y[train_index]\n",
    "        y_test = y[test_index]\n",
    "        \n",
    "        class_ratio = np.sum(y_train == 0)/np.sum(y_train == 1)\n",
    "            \n",
    "        model = XGBClassifier(scale_pos_weight=class_ratio)\n",
    "        model.set_params(**params)\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        probs_test = model.predict_proba(X_test)[:,1]\n",
    "\n",
    "        testing_avp = sklearn.metrics.average_precision_score(y_test, probs_test)\n",
    "\n",
    "        mavps.append(testing_avp)\n",
    "\n",
    "    return -np.mean(mavps)\n",
    "                    \n",
    "res_gp = gp_minimize(mean_average_precision_of_RF, space, n_calls=10, random_state=0)\n",
    "\n",
    "print(\"Best score=%.4f\" % res_gp.fun)\n",
    "print(\"Hyperparameter Values\", res_gp.x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
