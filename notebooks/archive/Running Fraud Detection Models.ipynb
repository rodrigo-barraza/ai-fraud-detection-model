{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for auto-reloading extensions - helpful if you're writing and testing a package\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# for inline plotting in python using matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# for easier plots - also makes matplotlib plots look nicer by default\n",
    "import seaborn as sns\n",
    "\n",
    "# set up for using plotly offline without an API key - great for interactive plots\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "import plotly.graph_objs as go\n",
    "import plotly.figure_factory as ff\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "# for numerical work\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pymongo\n",
    "\n",
    "import datetime\n",
    "import time\n",
    "import json\n",
    "\n",
    "from pandas.io.json import json_normalize\n",
    "from pymongo import MongoClient\n",
    "\n",
    "import pickle\n",
    "\n",
    "from confluent_kafka import Producer\n",
    "\n",
    "import bson\n",
    "from bson import json_util\n",
    "\n",
    "import math\n",
    "\n",
    "# load the database credentials from file\n",
    "with open('../creds/creds.json') as json_data:\n",
    "    creds = json.load(json_data)\n",
    "    \n",
    "client = MongoClient(creds['connection_string'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries = list(client['ml']['requestEvents60Summaries'].find())\n",
    "\n",
    "whitelist = [item['email'] for item in client['production']['emailWhitelistCollection'].find() if item['level'] == 'ALLOWED']\n",
    "blacklist = [item['email'] for item in client['production']['emailBlacklistCollection'].find() if item['level'] == 'BLOCKED']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_whitelist(email):\n",
    "    \n",
    "    if email in whitelist:\n",
    "        return True\n",
    "    if any(string in email.lower() for string in ['fingerfood', 'einstein.exchange', 'test', 'testing']):\n",
    "        return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "def is_blacklist(email):\n",
    "    \n",
    "    if email in blacklist:\n",
    "        return 1\n",
    "    \n",
    "    return 0\n",
    "\n",
    "def drop_whitelist(df):\n",
    "\n",
    "    return df[df['request.user_email'].apply(is_whitelist) == False]\n",
    "\n",
    "def label_fraud(df):\n",
    "\n",
    "    df['fraud'] = df['request.user_email'].apply(is_blacklist)\n",
    "    \n",
    "    return df[df['request.user_email'].apply(is_whitelist) == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = label_fraud(drop_whitelist(json_normalize(summaries)))\n",
    "sdf = sdf.fillna(0)\n",
    "sdf.groupby(['request.user_email','fraud'], as_index=False)._id.count().sort_values('_id', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot = ['request.category_action','request.category_action_label','request.category_label',\n",
    "           'request.event_action','request.event_category','request.event_label',\n",
    "           'request.fiat_currency','request.cryptocurrency','request.transaction_type',\n",
    "           'request.user_city','request.user_country','request.user_province_state_territory']\n",
    "\n",
    "to_numeric = ['request.created']\n",
    "\n",
    "as_is = ['request.cryptocurrency_amount','request.fiat_currency_value','request.fiat_rate','request.user_email']\n",
    "\n",
    "keep_request_columns = sorted(one_hot + to_numeric + as_is)\n",
    "\n",
    "drop_columns = ['_id'] + [col for col in sdf.columns if 'request.' in col and col not in keep_request_columns]\n",
    "\n",
    "sdf.drop(drop_columns, axis=1, inplace=True)\n",
    "\n",
    "sdf = pd.get_dummies(sdf, columns=one_hot)\n",
    "\n",
    "for col in to_numeric:\n",
    "    sdf[col] = pd.to_numeric(sdf[col])\n",
    "\n",
    "for col in sorted(sdf.columns):\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(sdf[sdf.fraud == 1].n_events.fillna(0), hist=False, rug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(sdf[sdf.fraud == 0].n_events.fillna(0), hist=False, rug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(group_counts, n_splits, tolerance=0):\n",
    "    \n",
    "    max_split_size = math.ceil(group_counts['count'].sum()/n_splits)\n",
    "    \n",
    "    splits = {}\n",
    "    \n",
    "    # initialize the dict\n",
    "    for split in range(n_splits):\n",
    "        splits[split] = {'groups': [], 'n_examples': 0}\n",
    "    \n",
    "    for i, (group, count) in enumerate(zip(group_counts['group'].values, group_counts['count'].values)):\n",
    "        \n",
    "        rand_splits = np.random.choice(np.arange(n_splits), size=n_splits, replace=False)\n",
    "        \n",
    "        for split in rand_splits:\n",
    "            \n",
    "            new_count = splits[split]['n_examples']+count\n",
    "            \n",
    "            # if the current split is empty and the count is less than tolerance over the max_split_size\n",
    "            if count > max_split_size and splits[split]['n_examples'] == 0 and (count-max_split_size)/max_split_size <= tolerance:\n",
    "            \n",
    "                # if new_count > max_split_size and (new_count-max_split_size)/max_split_size <= tolerance:\n",
    "                splits[split]['groups'].append(group)\n",
    "                splits[split]['n_examples'] += count\n",
    "                \n",
    "                break\n",
    "            else:\n",
    "                if (max_split_size-splits[split]['n_examples']) >= count:\n",
    "                    splits[split]['groups'].append(group)\n",
    "                    splits[split]['n_examples'] += count\n",
    "                    break\n",
    "    \n",
    "    \n",
    "    return splits\n",
    "\n",
    "\n",
    "def balanced_group_n_fold(labels, groups, tolerance=0.1, max_splits=10):\n",
    "    \n",
    "    # determine the minority group class\n",
    "    df = pd.DataFrame({'label': labels, 'group': groups})\n",
    "    \n",
    "    # count the number of groups by class\n",
    "    result = df.groupby('label')['group'].agg(['count']).sort_values(by='count', ascending=True).reset_index()\n",
    "    display(result)\n",
    "    \n",
    "    # select the class with the fewest groups\n",
    "    minority_group_class = result['label'].values[0]\n",
    "    majority_group_class = result['label'].values[1]\n",
    "    \n",
    "    minority_df = df[df['label'] == minority_group_class]\n",
    "    majority_df = df[df['label'] == majority_group_class] # should check that there is indeed a second class\n",
    "\n",
    "    print(\"Minority Class: \", minority_group_class)\n",
    "    \n",
    "    # determine the max number of folds\n",
    "    # get the group with the most number of examples in the minority class\n",
    "    minority_group_counts = minority_df.groupby('group', as_index=False)['label'].agg(['count']).reset_index().sort_values(by='count', ascending=False)\n",
    "    majority_group_counts = majority_df.groupby('group', as_index=False)['label'].agg(['count']).reset_index().sort_values(by='count', ascending=False)\n",
    "\n",
    "    max_folds = minority_df.shape[0]/minority_group_counts['count'].values[0]\n",
    "    max_folds = math.ceil(max_folds) if max_folds%1 >= (1-tolerance) else math.floor(max_folds)\n",
    "    max_folds = min(max_folds, max_splits)\n",
    "    print(\"Number of folds is: \",max_folds)\n",
    "    \n",
    "    minority_splits = split(minority_group_counts, max_folds, tolerance=tolerance)\n",
    "    majority_splits = split(majority_group_counts, max_folds, tolerance=tolerance)\n",
    "    \n",
    "    combined = {}\n",
    "    \n",
    "    # combine the minority and majority groups together\n",
    "    for key in minority_splits.keys():\n",
    "        combined[key] = {'groups': minority_splits[key]['groups'] + majority_splits[key]['groups'],\n",
    "                         'n_examples': minority_splits[key]['n_examples'] + majority_splits[key]['n_examples']}\n",
    "        \n",
    "    for key in combined.keys():\n",
    "        print(\"Minority Class Fold:\", key, ', groups:', len(minority_splits[key]['groups']), ', n_examples:', minority_splits[key]['n_examples'])\n",
    "        print(\"Majority Class Fold:\", key, ', groups:', len(majority_splits[key]['groups']), ', n_examples:', majority_splits[key]['n_examples'])\n",
    "        print(\"Combine Fold:\", key, ', groups:', len(combined[key]['groups']), ', n_examples:', combined[key]['n_examples'])\n",
    "    \n",
    "    splits = []\n",
    "    \n",
    "    # set up the indices\n",
    "    indices = np.arange(len(labels))\n",
    "    \n",
    "    # create the plit indices\n",
    "    for key in combined.keys():\n",
    "        splits.append((indices[np.isin(groups, combined[key]['groups']) == False],indices[np.isin(groups, combined[key]['groups'])]))\n",
    "    \n",
    "    result = splits\n",
    "    \n",
    "    return result\n",
    "\n",
    "    \n",
    "folds = balanced_group_n_fold(sdf['fraud'], sdf['request.user_email'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossValidationResults(df, labels, groups, classifier_list):\n",
    "    \n",
    "    # variables to keep track of\n",
    "    training_fraud_percentages = []\n",
    "    testing_fraud_percentages = []\n",
    "    training_accuracies = []\n",
    "    testing_accuracies = []\n",
    "    training_average_precisions = []\n",
    "    testing_average_precisions = []\n",
    "    train_true_negatives = []\n",
    "    train_false_negatives = []\n",
    "    train_true_positives = []\n",
    "    train_false_positives = []\n",
    "    test_true_negatives = []\n",
    "    test_false_negatives = []\n",
    "    test_true_positives = []\n",
    "    test_false_positives = []\n",
    "    classifiers = []\n",
    "    \n",
    "    X = df.drop([labels,groups], axis=1).astype('float32')\n",
    "    X[X == -np.inf] = 0\n",
    "    X[X == np.inf] = 0\n",
    "    X = X.values\n",
    "    \n",
    "    y = df[labels].values\n",
    "    groups = df[groups].values\n",
    "\n",
    "    cv_folds = balanced_group_n_fold(y, groups)\n",
    "    \n",
    "    # for each classifer type\n",
    "    for clf in classifier_list:\n",
    "        \n",
    "        classifier = clf['Label']\n",
    "        model = clf['Model']\n",
    "\n",
    "        for train_index, test_index in cv_folds:\n",
    "\n",
    "            X_train = X[train_index,:]\n",
    "            X_test = X[test_index,:]\n",
    "            y_train = y[train_index]\n",
    "            y_test = y[test_index]\n",
    "\n",
    "            class_ratio = np.sum(y_train == 0)/np.sum(y_train == 1)\n",
    "\n",
    "#             if type(model) == type(XGBClassifier()):\n",
    "#                 model.set_params(**{'scale_pos_weight': class_ratio})\n",
    "\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            preds_train = model.predict(X_train)\n",
    "            preds_test = model.predict(X_test)\n",
    "            probs_train = model.predict_proba(X_train)[:,1]\n",
    "            probs_test = model.predict_proba(X_test)[:,1]\n",
    "\n",
    "            training_accuracy = np.sum(preds_train == y_train)/len(y_train)\n",
    "            testing_accuracy = np.sum(preds_test == y_test)/len(y_test)\n",
    "\n",
    "            training_avp = sklearn.metrics.average_precision_score(y_train, probs_train)\n",
    "            testing_avp = sklearn.metrics.average_precision_score(y_test, probs_test)\n",
    "\n",
    "            train_cm = sklearn.metrics.confusion_matrix(y_train, preds_train, sample_weight=None)\n",
    "\n",
    "            test_cm = sklearn.metrics.confusion_matrix(y_test, preds_test, sample_weight=None)\n",
    "\n",
    "            training_fraud_percentages.append(np.sum(y_train)/len(y_train))\n",
    "            testing_fraud_percentages.append(np.sum(y_test)/len(y_test))\n",
    "            training_accuracies.append(training_accuracy)\n",
    "            testing_accuracies.append(testing_accuracy)\n",
    "            training_average_precisions.append(training_avp)\n",
    "            testing_average_precisions.append(testing_avp)\n",
    "            train_true_negatives.append(train_cm[0,0])\n",
    "            train_false_negatives.append(train_cm[1,0])\n",
    "            train_true_positives.append(train_cm[1,1])\n",
    "            train_false_positives.append(train_cm[0,1])\n",
    "            test_true_negatives.append(test_cm[0,0])\n",
    "            test_false_negatives.append(test_cm[1,0])\n",
    "            test_true_positives.append(test_cm[1,1])\n",
    "            test_false_positives.append(test_cm[0,1])\n",
    "            classifiers.append(classifier)\n",
    "\n",
    "    \n",
    "    # put results together in a dataframe\n",
    "    model_results_df = pd.DataFrame({\n",
    "        'classifier': classifiers,\n",
    "        'train_fraud_percentage': training_fraud_percentages,\n",
    "        'test_fraud_percentage': testing_fraud_percentages,\n",
    "        'train_accuracy': training_accuracies,\n",
    "        'test_accuracy': testing_accuracies,\n",
    "        'train_average_precision': training_average_precisions,\n",
    "        'test_average_precision': testing_average_precisions,\n",
    "        'train_true_negatives': train_true_negatives,\n",
    "        'train_false_negatives': train_false_negatives,\n",
    "        'train_true_positives': train_true_positives,\n",
    "        'train_false_positives': train_false_positives,\n",
    "        'test_true_negatives': test_true_negatives,\n",
    "        'test_false_negatives': test_false_negatives,\n",
    "        'test_true_positives': test_true_positives,\n",
    "        'test_false_positives': test_false_positives,\n",
    "    })     \n",
    "\n",
    "    # calculate precision and recall\n",
    "    model_results_df['test_precision'] = model_results_df.test_true_positives/(model_results_df.test_true_positives+model_results_df.test_false_positives)\n",
    "    model_results_df['test_recall'] = model_results_df.test_true_positives/(model_results_df.test_true_positives+model_results_df.test_false_negatives)\n",
    "    model_results_df['train_precision'] = model_results_df.train_true_positives/(model_results_df.train_true_positives+model_results_df.train_false_positives)\n",
    "    model_results_df['train_recall'] = model_results_df.train_true_positives/(model_results_df.train_true_positives+model_results_df.train_false_negatives)\n",
    "    model_results_df.fillna(0, inplace=True)\n",
    "\n",
    "    return model_results_df[sorted(model_results_df.columns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#from xgboost import XGBClassifier\n",
    "\n",
    "classifiers = [\n",
    "    {'Label': 'Random Forest', 'Model': RandomForestClassifier()},\n",
    "    {'Label': 'Modified Random Forest', 'Model': RandomForestClassifier(n_estimators=100, max_depth=5, max_features=int(sdf.shape[1]/100), class_weight='balanced')},\n",
    "    {'Label': 'Dummy', 'Model': DummyClassifier()}\n",
    "]\n",
    "\n",
    "model_results = crossValidationResults(df=sdf, labels='fraud', groups='request.user_email', classifier_list=classifiers)\n",
    "        \n",
    "summary = model_results.groupby(['classifier'], as_index=False)[['test_precision','test_recall','test_average_precision']].agg(['mean','median','std']).reset_index()\n",
    "summary.columns = [col[0] if col[1] == '' else '_'.join(col) for col in summary.columns.ravel()]\n",
    "summary.sort_values(by='test_average_precision_mean', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in sdf.columns: print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = [\n",
    "    {'Label': 'Random Forest', 'Model': RandomForestClassifier()},\n",
    "    {'Label': 'Modified Random Forest', 'Model': RandomForestClassifier(n_estimators=100, max_depth=5, max_features=int(sdf.shape[1]/100), class_weight='balanced')},\n",
    "    {'Label': 'Dummy', 'Model': DummyClassifier()}\n",
    "]\n",
    "\n",
    "model_results = crossValidationResults(df=sdf., labels='fraud', groups='request.user_email', classifier_list=classifiers)\n",
    "        \n",
    "summary = model_results.groupby(['classifier'], as_index=False)[['test_precision','test_recall','test_average_precision']].agg(['mean','median','std']).reset_index()\n",
    "summary.columns = [col[0] if col[1] == '' else '_'.join(col) for col in summary.columns.ravel()]\n",
    "summary.sort_values(by='test_average_precision_mean', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sdf[sdf['request.transaction_type_interac'] == True]\n",
    "\n",
    "classifiers = [\n",
    "    {'Label': 'Random Forest', 'Model': RandomForestClassifier()},\n",
    "    {'Label': 'Modified Random Forest', 'Model': RandomForestClassifier(n_estimators=100, max_depth=5, max_features=int(data.shape[1]/100), class_weight='balanced')},\n",
    "    {'Label': 'Dummy', 'Model': DummyClassifier()}\n",
    "]\n",
    "\n",
    "model_results = crossValidationResults(df=data, labels='fraud', groups='request.user_email', classifier_list=classifiers)\n",
    "        \n",
    "summary = model_results.groupby(['classifier'], as_index=False)[['test_precision','test_recall','test_average_precision']].agg(['mean','median','std']).reset_index()\n",
    "summary.columns = [col[0] if col[1] == '' else '_'.join(col) for col in summary.columns.ravel()]\n",
    "summary.sort_values(by='test_average_precision_mean', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sdf[sdf['request.transaction_type_credit card'] == True]\n",
    "\n",
    "classifiers = [\n",
    "    {'Label': 'Random Forest', 'Model': RandomForestClassifier()},\n",
    "    {'Label': 'Modified Random Forest', 'Model': RandomForestClassifier(n_estimators=100, max_depth=5, max_features=int(data.shape[1]/100), class_weight='balanced')},\n",
    "    {'Label': 'Dummy', 'Model': DummyClassifier()}\n",
    "]\n",
    "\n",
    "model_results = crossValidationResults(df=data, labels='fraud', groups='request.user_email', classifier_list=classifiers)\n",
    "        \n",
    "summary = model_results.groupby(['classifier'], as_index=False)[['test_precision','test_recall','test_average_precision']].agg(['mean','median','std']).reset_index()\n",
    "summary.columns = [col[0] if col[1] == '' else '_'.join(col) for col in summary.columns.ravel()]\n",
    "summary.sort_values(by='test_average_precision_mean', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
