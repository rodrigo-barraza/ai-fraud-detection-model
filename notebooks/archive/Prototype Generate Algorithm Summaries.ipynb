{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for numerical work\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pymongo\n",
    "\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "from pandas.io.json import json_normalize\n",
    "from pymongo import MongoClient\n",
    "\n",
    "import pickle\n",
    "\n",
    "import re\n",
    "\n",
    "\n",
    "# load the database credentials from file\n",
    "with open('../creds/creds.json') as json_data:\n",
    "    creds = json.load(json_data)\n",
    "    \n",
    "client = MongoClient(creds['connection_string'])\n",
    "\n",
    "# connect to the ml mongo database\n",
    "ml = client['ml']\n",
    "\n",
    "# check if collection exists and if not create it\n",
    "if 'requestEvents60Summaries' not in list(ml.list_collection_names()):\n",
    "    \n",
    "    # create the collection \n",
    "    ml.create_collection('requestEvents60Summaries')\n",
    "    \n",
    "latest_record_list = list(ml['requestEvents60Summaries'].find().sort('request_created',-1).limit(1))\n",
    "\n",
    "# if there's a latest record\n",
    "if len(latest_record_list) > 0:\n",
    "    \n",
    "    # get the latest record created date\n",
    "    latest_record_time = latest_record_list[0]['request_created']\n",
    "    \n",
    "    print('Most recent request time:', latest_record_time)\n",
    "    \n",
    "        \n",
    "    # get the new credit card and interac requests\n",
    "    all_requests = list(client['ml']['requestEvents60'].find({'request.created': {'$gte': latest_record_time}}))\n",
    "\n",
    "# get them all\n",
    "else:\n",
    "    print(\"No pre-processed records, processing all.\")\n",
    "    # get the new credit card and interac requests\n",
    "    all_requests = list(client['ml']['requestEvents60'].find())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if there's new requests to process\n",
    "if len(all_requests) > 0:\n",
    "    # flatten into one object per event\n",
    "    flat_requests = [{'request': rs['request'], \n",
    "                      'event': event} for rs in all_requests for event in rs['events']]\n",
    "\n",
    "    # flatten requests into a dataframe\n",
    "    all_events = pd.DataFrame(json_normalize(flat_requests))\n",
    "\n",
    "    # create a dataframe with the results\n",
    "    df_with_id = all_events\n",
    "\n",
    "    # replace infinity and Nan strings with proper np.nan\n",
    "    df_with_id.replace(['NaN','Infinity'],np.nan, inplace=True)\n",
    "\n",
    "    # sort by request id and date\n",
    "    df_with_id = df_with_id.sort_values(by=['request._id','event.created'])\n",
    "\n",
    "    # calculate the previous event time and the time between events\n",
    "    df_with_id['previous_event_time'] = df_with_id.groupby(['request._id'])['event.created'].shift(1)\n",
    "    df_with_id['event.time_since_last_event'] = pd.to_numeric(df_with_id['event.created']-df_with_id['previous_event_time'])*1e-9\n",
    "\n",
    "    # replace string versions of infinity with proper inf object\n",
    "    df_with_id = df_with_id.replace('Infinity', np.inf)\n",
    "\n",
    "    # convert columns that should be to numeric\n",
    "    df_with_id['request.metadata.amount'] = pd.to_numeric(df_with_id['request.metadata.amount'])\n",
    "    df_with_id['request.metadata.rate'] = pd.to_numeric(df_with_id['request.metadata.rate'])\n",
    "    df_with_id['request.metadata.cents'] = pd.to_numeric(df_with_id['request.metadata.cents'])\n",
    "    df_with_id['request.value'] = pd.to_numeric(df_with_id['request.value'])\n",
    "    df_with_id['event.metadata.amount'] = pd.to_numeric(df_with_id['event.metadata.amount'])\n",
    "    df_with_id['event.metadata.rate'] = pd.to_numeric(df_with_id['event.metadata.rate'])\n",
    "    df_with_id['event.metadata.cents'] = pd.to_numeric(df_with_id['event.metadata.cents'])\n",
    "    df_with_id['event.value'] = pd.to_numeric(df_with_id['event.value'])\n",
    "\n",
    "    # get the days since november\n",
    "    df_with_id['event.days_since_nov'] = df_with_id['event.created'].apply(lambda x: (x - datetime.datetime(year=2017,month=11,day=1)).days)\n",
    "\n",
    "    # replace older bitcoin labels with new format\n",
    "    df_with_id.loc[df_with_id['event.eventLabel'].str.lower() == 'bitcoin', 'event.eventLabel'] = 'BTC'\n",
    "\n",
    "    # create unique category identifiers\n",
    "    df_with_id['event.category_action_label'] = df_with_id['event.eventCategory']+'_'+df_with_id['event.eventAction']+'_'+df_with_id['event.eventLabel']\n",
    "    df_with_id['event.category_action'] = df_with_id['event.eventCategory']+'_'+df_with_id['event.eventAction']\n",
    "\n",
    "    # drop columns that contain list/array values because they can't be processed\n",
    "    list_drops = [col for col in df_with_id.columns if df_with_id[col].apply(lambda x: type(x)).value_counts().index[0] == \"<class 'list'>\"]\n",
    "    df_with_id = df_with_id.drop(list_drops, axis=1)\n",
    "\n",
    "    # drop some other columns\n",
    "    df_with_id = df_with_id.drop(['event.metadata.authResponseEIN.body.data.token_type','event.metadata.authResponseEIN.headers.map.content-type'], axis=1)\n",
    "\n",
    "    # categorical columns that need to be converted to binary\n",
    "    categorical_columns = ['event.category_action',\n",
    "                            'event.category_action_label',\n",
    "                            'event.metadata.addressCity',\n",
    "                            'event.metadata.addressCountry',\n",
    "                            'event.metadata.addressProvince',\n",
    "                            'event.metadata.city',\n",
    "                            'event.metadata.country',\n",
    "                            'event.metadata.currency',\n",
    "                            'event.metadata.instrument',\n",
    "                            'event.metadata.mongoResponse.product',\n",
    "                            'event.metadata.product',\n",
    "                            'event.metadata.productId',\n",
    "                            'event.metadata.prossessorError.billingDetails.city',\n",
    "                            'event.metadata.prossessorError.billingDetails.country',\n",
    "                            'event.metadata.prossessorError.billingDetails.state',\n",
    "                            'event.metadata.prossessorError.card.type',\n",
    "                            'event.metadata.prossessorError.currencyCode',\n",
    "                            'event.metadata.prossessorResponse.billingDetails.city',\n",
    "                            'event.metadata.prossessorResponse.billingDetails.country',\n",
    "                            'event.metadata.prossessorResponse.billingDetails.province',\n",
    "                            'event.metadata.prossessorResponse.billingDetails.state',\n",
    "                            'event.metadata.prossessorResponse.card.cardType',\n",
    "                            'event.metadata.prossessorResponse.card.type',\n",
    "                            'event.metadata.prossessorResponse.card_type',\n",
    "                            'event.metadata.prossessorResponse.currency',\n",
    "                            'event.metadata.prossessorResponse.currencyCode',\n",
    "                            'event.metadata.province',\n",
    "                            'event.metadata.requestParams.currency',\n",
    "                            'event.metadata.requestParams.product',\n",
    "                            'event.metadata.type']\n",
    "\n",
    "\n",
    "    unique_columns = ['event.metadata.bankName',\n",
    "                     'event.metadata.cardHolder',\n",
    "                     'event.metadata.cardId',\n",
    "                     'event.metadata.cardName',\n",
    "                     'event.metadata.cardNumberLastFour',\n",
    "                     'event.metadata.cardPrefix',\n",
    "                     'event.metadata.cardSuffix',\n",
    "                     'event.metadata.email',\n",
    "                     'event.metadata.firstName',\n",
    "                     'event.metadata.fullName',\n",
    "                     'event.metadata.lastName',\n",
    "                     'event.metadata.mongoResponse.email',\n",
    "                     'event.metadata.name',\n",
    "                     'event.metadata.prossessorError.card.cardExpiry.month',\n",
    "                     'event.metadata.prossessorError.card.cardExpiry.year',\n",
    "                     'event.metadata.prossessorError.card.lastDigits',\n",
    "                     'event.metadata.prossessorError.card.type',\n",
    "                     'event.metadata.prossessorResponse.card.cardExpiry.month',\n",
    "                     'event.metadata.prossessorResponse.card.cardExpiry.year',\n",
    "                     'event.metadata.prossessorResponse.card.cardType',\n",
    "                     'event.metadata.prossessorResponse.card.lastDigits',\n",
    "                     'event.metadata.prossessorResponse.card.type',\n",
    "                     'event.metadata.prossessorResponse.card_expiry_month',\n",
    "                     'event.metadata.prossessorResponse.card_expiry_year',\n",
    "                     'event.metadata.prossessorResponse.card_suffix',\n",
    "                     'event.metadata.prossessorResponse.card_type',\n",
    "                     'event.metadata.prossessorResponse.profile.email',\n",
    "                     'event.metadata.prossessorResponse.profile.firstName',\n",
    "                     'event.metadata.prossessorResponse.profile.lastName',\n",
    "                     'event.metadata.requestParams.card_id',\n",
    "                     'event.metadata.requestParams.email']\n",
    "\n",
    "    numerical_per_currency = ['event.metadata.amount',\n",
    "                             'event.metadata.blockioResponse.data.amount_sent',\n",
    "                             'event.metadata.blockioResponse.data.amount_withdrawn',\n",
    "                             'event.metadata.lastTradedPx',\n",
    "                             'event.metadata.mongoResponse.amount',\n",
    "                             'event.metadata.mongoResponse.price',\n",
    "                             'event.metadata.price',\n",
    "                             'event.metadata.prossessorResponse.amount',\n",
    "                             'event.metadata.rate',\n",
    "                             'event.metadata.requestParams.amount',\n",
    "                             'event.metadata.requestParams.price',\n",
    "                             'event.metadata.requestParams.product_amount']\n",
    "\n",
    "    numerical_overall = ['event.metadata.cents',\n",
    "                         'event.metadata.prossessorResponse.charge_amount',\n",
    "                         'event.metadata.requestParams.charge_amount',\n",
    "                         'event.value',\n",
    "                         'event.time_since_last_event',\n",
    "                         'event.days_since_nov']\n",
    "\n",
    "    all_columns = list(set(categorical_columns + numerical_per_currency + numerical_overall + unique_columns))\n",
    "\n",
    "    # convert columns to either numeric or categorical\n",
    "    def convert_to_numeric_or_lower_str(column):\n",
    "\n",
    "            try:\n",
    "                return pd.to_numeric(column)\n",
    "\n",
    "            except:\n",
    "                return column.str.lower()\n",
    "\n",
    "    # summarize the columns where unique values matter\n",
    "    unique_data = df_with_id[['request._id']+unique_columns]\n",
    "\n",
    "\n",
    "    def n_unique(series):\n",
    "        '''Function to map over a Pandas series to get the number of unique elements.'''\n",
    "        return series.dropna().unique().size\n",
    "\n",
    "\n",
    "    def n_NaN(series):\n",
    "        '''Function to map over a pandas columns ot get the number of NaN elements.'''\n",
    "        return np.sum(series.isnull())\n",
    "\n",
    "    # summarize the columns where uniqueness matters\n",
    "    unique_summary = unique_data.groupby(['request._id'])[unique_columns].agg([n_unique, n_NaN])\n",
    "    unique_summary.columns = [col[0] if col[1] == '' else col[0]+'_'+col[1] for col in unique_summary.columns.ravel()]\n",
    "\n",
    "    # summarize the columns where they are numerical but the values are specific to a currency such as the metadata.amount field.\n",
    "    numerical_by_currency_data = df_with_id[['request._id','event.category_action_label']+numerical_per_currency]\n",
    "    numerical_by_currency_data.dropna(axis=0, how='all', subset=numerical_per_currency, inplace=True)\n",
    "    groupby_agg = numerical_by_currency_data.groupby(['request._id','event.category_action_label'], as_index=False)[numerical_per_currency].agg(['mean','median','max','min','std'])\n",
    "    groupby_agg.columns = [col[0] if col[1] == '' else col[0]+'_'+col[1] for col in groupby_agg.columns.ravel()]\n",
    "    groupby_agg = groupby_agg.reset_index()\n",
    "    groupby_agg = groupby_agg.melt(id_vars=['request._id','event.category_action_label'])\n",
    "    groupby_agg['variable'] = groupby_agg['event.category_action_label']+'_'+groupby_agg['variable'].astype(str)\n",
    "    groupby_agg.drop('event.category_action_label', axis=1, inplace=True)\n",
    "    groupby_agg = groupby_agg.pivot(index='request._id', columns='variable', values='value').reset_index().set_index('request._id')\n",
    "    numerical_by_currency_summary = groupby_agg\n",
    "\n",
    "    # summarize the columns that are numeric in nature and not specific to a given currency like the value\n",
    "    numerical_overall_data = df_with_id[['request._id']+numerical_overall]\n",
    "    numerical_overall_data.dropna(axis=0, how='all', subset=numerical_overall, inplace=True)\n",
    "    groupby_agg = numerical_overall_data.groupby(['request._id'])[numerical_overall].agg(['mean','median','max','min','std'])\n",
    "    groupby_agg.columns = [col[0] if col[1] == '' else col[0]+'_'+col[1] for col in groupby_agg.columns.ravel()]\n",
    "    numerical_overall_summary = groupby_agg\n",
    "\n",
    "    # summarize the data the is categorical in nature - it needs to be converted to binary format.\n",
    "    categorical_data = df_with_id[['request._id']+categorical_columns]\n",
    "\n",
    "    def string_to_lower(col):\n",
    "        '''Function to map over a Pandas series that tries to convert the column to a lowercase string column.'''\n",
    "        try:\n",
    "            return col.str.lower()\n",
    "        except:\n",
    "            return col\n",
    "\n",
    "    categorical_data = categorical_data.apply(string_to_lower, axis=1)\n",
    "\n",
    "    categorical_data = pd.get_dummies(categorical_data, columns=categorical_columns, dummy_na=True)\n",
    "    groupby_agg = categorical_data.groupby(['request._id']).agg(['sum'])\n",
    "    groupby_agg.columns = [col[0] if col[1] == '' else col[0]+'_'+col[1] for col in groupby_agg.columns.ravel()]\n",
    "\n",
    "    categorical_data_summary = groupby_agg\n",
    "\n",
    "    # combine all the summaries together into one column\n",
    "    data_summary = unique_summary.join(numerical_by_currency_summary).join(numerical_overall_summary).join(categorical_data_summary)\n",
    "\n",
    "    # format the actual request data \n",
    "    requests = [rs['request'] for rs in all_requests]\n",
    "    requests_df = pd.DataFrame(json_normalize(requests))\n",
    "    requests_df.columns = ['request.'+col for col in requests_df.columns]\n",
    "    request_df = pd.get_dummies(requests_df[['request._id','request.metadata.email','request.created','request.eventCategory']], columns=['request.eventCategory'])\n",
    "\n",
    "    # join the request data with the pre-request data\n",
    "    data = request_df.set_index('request._id').join(data_summary)\n",
    "    \n",
    "    def convert(name):\n",
    "        s1 = re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', name)\n",
    "        return re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', s1).lower().replace('.','_').replace('__','_')\n",
    "    \n",
    "    # change camelcase to snake case and remove periods in column names \n",
    "    data.columns = [convert(col) for col in data.columns]\n",
    "    \n",
    "#     # convert pandas dataframe to json\n",
    "#     data_json = data.to_dict(orient='records')\n",
    "    \n",
    "#     requestEvents60Summaries = client['ml']['requestEvents60Summaries']\n",
    "#     requestEvents60Summaries.insert_many(data_json)\n",
    "\n",
    "else:\n",
    "    print('No new requests to process.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Data for The Actual Machine Learning Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_request_sets = list(client['ml']['requestEvents60'].find().limit(1000))\n",
    "flat_requests = [{'request': r['request'], 'event': e} for r in all_request_sets for e in r['events']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat = json_normalize(flat_requests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[col for col in flat.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the fraudulent emails\n",
    "def remove_whitelist_emails(df, email_col):\n",
    "    '''Remove whitelisted or test emails'''\n",
    "\n",
    "    # get all the events related to the requests aka within 60 minutes before the first request for the users who mader requests\n",
    "    wl_emails = [r['email'] for r in list(client['production']['emailWhitelistCollection'].find({'level': 'BLOCKED'}))]\n",
    " \n",
    "    return df[df['request.metadata.email'].isin(wl_emails) == False]\n",
    "\n",
    "# get the fraudulent emails\n",
    "def get_fraud_labels(user_emails):\n",
    "    '''Remove whitelisted or test emails'''\n",
    "\n",
    "    # get all the events related to the requests aka within 60 minutes before the first request for the users who mader requests\n",
    "    bl_emails = [r['email'] for r in list(client['production']['emailBlacklistCollection'].find({'level': 'BLOCKED'}))]\n",
    " \n",
    "    return np.array([1 if user in bl_emails else 0 for user in user_emails])\n",
    "\n",
    "# remove whitelist emails\n",
    "data = remove_whitelist_emails(data, 'request.metadata.email')\n",
    "\n",
    "# get the fraud labels\n",
    "data['fraud'] = get_fraud_labels(data['request.metadata.email'])\n",
    "\n",
    "# fill na values with zero\n",
    "data = data.fillna(0)\n",
    "\n",
    "print(\"Dataframe is\",np.sum(data.memory_usage())*1e-9,'gigabytes in memory')\n",
    "\n",
    "print(\"Saving to hdf5 file\")\n",
    "\n",
    "data.to_hdf('../lstm_data_prep_pipeline/results/all_request_summaries.hdf5', 'table')\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    'model': 'Previous 60 Minutes Random Forest',\n",
    "    'created': 'date the model was created'\n",
    "    'n_examples': 11000\n",
    "    'n_fraudulent': 1000\n",
    "    'n_notfraudulent': 10000\n",
    "    'training_time': 5 mins\n",
    "    'training_type': 'full' or 'update'\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
