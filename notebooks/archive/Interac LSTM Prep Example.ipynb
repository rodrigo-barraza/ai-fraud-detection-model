{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard snippets I use a lot\n",
    "\n",
    "# for auto-reloading extensions - helpful if you're writing and testing a package\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# for inline plotting in python using matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# for easier plots - also makes matplotlib plots look nicer by default\n",
    "import seaborn as sns\n",
    "\n",
    "# set up for using plotly offline without an API key - great for interactive plots\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "import plotly.graph_objs as go\n",
    "import plotly.figure_factory as ff\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "# for numerical work\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pymongo\n",
    "\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "from pandas.io.json import json_normalize\n",
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the database credentials from file\n",
    "with open('../creds/creds.json') as json_data:\n",
    "    creds = json.load(json_data)\n",
    "\n",
    "# set up a database with credentials\n",
    "client = MongoClient(creds['connection_string'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download all events from the mongo database\n",
    "all_events_json = list(client['production']['eventCollection'].find())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten the events\n",
    "all_events_flat = json_normalize(all_events_json)\n",
    "\n",
    "# convert to a pandas dataframe\n",
    "all_events = pd.DataFrame(all_events_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the data to a file so if the kernel crashes you don't need to re-read from the database\n",
    "# need to have a data directory inside notebooks\n",
    "all_events = all_events.to_csv('data/all_events.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the saved data instead of reading from the database\n",
    "all_events = pd.read_csv('data/all_events.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_events.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename the older 'bitcoin' events to BTC so they match\n",
    "all_events.loc[all_events.eventLabel.str.lower() == 'bitcoin', 'eventLabel'] = 'BTC'\n",
    "\n",
    "# convert created to a datetime instead of a string\n",
    "all_events['created'] = pd.to_datetime(all_events['created'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all of the interac requests\n",
    "interac_requests = all_events[(all_events['metadata.email'].isnull() == False) & (all_events.eventCategory == 'buy') & (all_events.eventAction == 'request')].reset_index(drop=True)\n",
    "\n",
    "def flagFraudsters(df):\n",
    "\n",
    "    blemails = list(pd.DataFrame(json_normalize(list(client['production']['emailBlacklistCollection'].find({'level': 'BLOCKED'})))).email) + ['gaelkevin@hotmail.com', 'royer.8383@gmail.com','adventurous7381@gmail.com']\n",
    "    \n",
    "    df['fraud'] = df['metadata.email'].isin(blemails)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def removeWhitelistRecords(df):\n",
    "\n",
    "    wlemails = pd.DataFrame(json_normalize(list(client['production']['emailWhitelistCollection'].find({'level': 'ALLOWED'})))).email\n",
    "    \n",
    "    df = df[df['metadata.email'].str.contains('test') == False]\n",
    "    df = df[df['metadata.email'].str.contains('fingerfoodstudios') == False]\n",
    "    df = df[df['metadata.email'].str.contains('einstein.exchange') == False]    \n",
    "    df = df[df['metadata.email'].isin(wlemails) == False]\n",
    "    \n",
    "    return df \n",
    "\n",
    "# flag the fraudulnet records and remove the whitelist and test accounts\n",
    "interac_requests = removeWhitelistRecords(flagFraudsters(interac_requests))\n",
    "\n",
    "display(interac_requests.head())\n",
    "interac_requests.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = interac_requests[['_id','metadata.email','created','metadata.amount','metadata.rate','value']]\n",
    "\n",
    "def subset_by_request(row):\n",
    "    \n",
    "    # get the email from the row\n",
    "    email = row['metadata.email']\n",
    "    \n",
    "    # get the created time from the row\n",
    "    created = row['created']\n",
    "    \n",
    "    # get the id from the row\n",
    "    _id = row['_id']\n",
    "    \n",
    "    # time to lookback an hour prior to the request\n",
    "    lookbacktime = created-datetime.timedelta(seconds=60*60) # one hour \n",
    "    \n",
    "    # get the events for this user where the time is before the request but not later than an hour before the request\n",
    "    events = all_events[(all_events['metadata.email'] == email) & (all_events['created'] <= created) & (all_events['created'] >= lookbacktime)]\n",
    "    \n",
    "    # give them a request_id for later group by operations\n",
    "    events['request_id'] = _id\n",
    "    \n",
    "    # convert the dataframe to a list of json records\n",
    "    return events.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the interac results to a dict\n",
    "result_dict = result.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the list of events for each request\n",
    "subsets = list(map(subset_by_request, result_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(subsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten the subsets so they aren't nested\n",
    "subsets_flat = [item for sublist in subsets for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe with the results\n",
    "df_with_id = pd.DataFrame(subsets_flat)\n",
    "\n",
    "# create the combined category action and category-action-label fields\n",
    "df_with_id['ca'] = df_with_id.eventCategory + '_' + df_with_id.eventAction\n",
    "df_with_id['cal'] = df_with_id.eventCategory + '_' + df_with_id.eventAction+ '_' + df_with_id.eventLabel\n",
    "\n",
    "# sort by request id and date\n",
    "df_with_id = df_with_id.sort_values(by=['request_id','created'])\n",
    "\n",
    "# calculate the previous event time and the time between events\n",
    "df_with_id['previous_event_time'] = df_with_id.groupby(['_id'])['created'].shift(1)\n",
    "df_with_id['time_since_last_event'] = pd.to_numeric(df_with_id['created']-df_with_id['previous_event_time'])*1e-9\n",
    "\n",
    "# replace string versions of infinity with proper inf object\n",
    "df_with_id = df_with_id.replace('Infinity', np.inf)\n",
    "\n",
    "# convert columns that should be to numeric\n",
    "df_with_id['metadata.amount'] = pd.to_numeric(df_with_id['metadata.amount'])\n",
    "df_with_id['metadata.rate'] = pd.to_numeric(df_with_id['metadata.rate'])\n",
    "df_with_id['metadata.cents'] = pd.to_numeric(df_with_id['metadata.cents'])\n",
    "df_with_id['value'] = pd.to_numeric(df_with_id['value'])\n",
    "\n",
    "df_with_id.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out all the available columns\n",
    "\n",
    "cols = []\n",
    "\n",
    "for col in sorted(list(df_with_id.columns)):\n",
    "    cols.append({'column': col, 'data_type': df_with_id[col].dtype})\n",
    "    \n",
    "with pd.option_context('display.max_rows', 500):\n",
    "    display(pd.DataFrame(cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the columns to keep\n",
    "columns_to_keep = ['request_id',\n",
    "                   'metadata.email', \n",
    "                   'created', \n",
    "                   'ca',\n",
    "                   'cal', \n",
    "                   'eventAction', \n",
    "                   'eventCategory', \n",
    "                   'eventLabel', \n",
    "                   'metadata.addressCity', \n",
    "                   'metadata.addressCountry', \n",
    "                   'metadata.addressProvince', \n",
    "                   'metadata.amount', \n",
    "                   'metadata.cents', \n",
    "                   'metadata.city', \n",
    "                   'metadata.country', \n",
    "                   'metadata.currency', \n",
    "                   'metadata.instrument', \n",
    "                   #'metadata.ip', \n",
    "                   'metadata.lastTradedPx', \n",
    "                   'metadata.mongoResponse.amount', \n",
    "                   #'metadata.mongoResponse.email', \n",
    "                   'metadata.mongoResponse.price', \n",
    "                   'metadata.mongoResponse.product', \n",
    "                   'metadata.price', \n",
    "                   'metadata.product', \n",
    "                   'metadata.prossessorError.billingDetails.city', \n",
    "                   'metadata.prossessorError.billingDetails.country', \n",
    "                   'metadata.prossessorError.billingDetails.state', \n",
    "                   'metadata.prossessorError.card.type', \n",
    "                   'metadata.prossessorResponse.billingDetails.city', \n",
    "                   'metadata.prossessorResponse.billingDetails.country', \n",
    "                   'metadata.prossessorResponse.billingDetails.province', \n",
    "                   'metadata.prossessorResponse.billingDetails.state', \n",
    "                   'metadata.prossessorResponse.card.type', \n",
    "                   'metadata.prossessorResponse.cardType', \n",
    "                   'metadata.prossessorResponse.card_type', \n",
    "                   'metadata.prossessorResponse.charge_amount', \n",
    "                   #'metadata.prossessorResponse.email', \n",
    "                   'metadata.province', \n",
    "                   'metadata.rate', \n",
    "                   'metadata.requestParams.amount', \n",
    "                   'metadata.requestParams.charge_amount', \n",
    "                   'metadata.requestParams.currency', \n",
    "                   #'metadata.requestParams.email', \n",
    "                   'metadata.requestParams.price', \n",
    "                   'metadata.requestParams.product', \n",
    "                   'metadata.requestParams.product_amount', \n",
    "                   'metadata.secondAmount', \n",
    "                   'metadata.tradesResponse', \n",
    "                   'metadata.type', \n",
    "                   #'previous_event_time',  \n",
    "                   'time_since_last_event', \n",
    "                   'value']\n",
    "\n",
    "# choose the columns to expand\n",
    "columns_to_expand = ['ca', \n",
    "                     'cal', \n",
    "                     'eventAction', \n",
    "                     'eventCategory', \n",
    "                     'eventLabel', \n",
    "                     'metadata.addressCity', \n",
    "                     'metadata.addressCountry', \n",
    "                     'metadata.addressProvince', \n",
    "                     'metadata.city', \n",
    "                     'metadata.country', \n",
    "                     'metadata.currency', \n",
    "                     'metadata.instrument', \n",
    "                     'metadata.mongoResponse.product', \n",
    "                     'metadata.product', \n",
    "                     'metadata.prossessorError.billingDetails.city', \n",
    "                     'metadata.prossessorError.billingDetails.country', \n",
    "                     'metadata.prossessorError.billingDetails.state', \n",
    "                     'metadata.prossessorError.card.type', \n",
    "                     'metadata.prossessorResponse.billingDetails.city', \n",
    "                     'metadata.prossessorResponse.billingDetails.country', \n",
    "                     'metadata.prossessorResponse.billingDetails.province', \n",
    "                     'metadata.prossessorResponse.billingDetails.state', \n",
    "                     'metadata.prossessorResponse.card.type', \n",
    "                     'metadata.prossessorResponse.cardType', \n",
    "                     'metadata.prossessorResponse.card_type', \n",
    "                     'metadata.province', \n",
    "                     'metadata.requestParams.currency', \n",
    "                     'metadata.requestParams.product', \n",
    "                     'metadata.tradesResponse', \n",
    "                     'metadata.type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the subset of columns\n",
    "subset = df_with_id[columns_to_keep]\n",
    "\n",
    "# fill inf values with na        \n",
    "subset = subset.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# create columns to track na status of each column\n",
    "for column in subset.columns:\n",
    "    if column not in ['request_id','metadata.email','created']:\n",
    "        subset[column+\"_na\"] = subset[column].isna()\n",
    "        \n",
    "# convert categorical columns to binary\n",
    "subset = pd.get_dummies(subset, columns=columns_to_expand)\n",
    "\n",
    "# fill na values with 0\n",
    "subset = subset.fillna(0)\n",
    "\n",
    "# convert the datetime to integer nanoseconds since 1970\n",
    "subset['created_utc_ns'] = pd.to_numeric(subset['created'])\n",
    "\n",
    "# sort ascending by request_id and descending by time\n",
    "subset = subset.sort_values(by=['request_id','created'], ascending=[True, False])\n",
    "\n",
    "# drop the time and email columns\n",
    "subset = subset.drop(['metadata.email','created'], axis = 1)\n",
    "subset = subset.reset_index(drop=True)\n",
    "\n",
    "# hacky way to number the sequence events prior to each interac request with 0 being the request and 10 being the 10th event prior to the request\n",
    "subset['index_int'] = subset.index\n",
    "event_index = subset.groupby('request_id')['index_int'].agg(lambda x: list(np.abs(min(x)-x)))\n",
    "all_index = [[item] if type(item) == type(int) else list(item) for item in event_index]\n",
    "all_index = [item for sublist in all_index for item in sublist]\n",
    "subset['timesteps'] = all_index\n",
    "\n",
    "# gather the columns together\n",
    "melt = subset.melt(id_vars=['request_id','timesteps'])\n",
    "\n",
    "# number of features is the number of columns at each timestep\n",
    "n_features = len(melt.variable.unique())\n",
    "\n",
    "# create new variable names that contain the timesteps\n",
    "melt['variable'] = melt.timesteps.astype(str) + \"_\" + melt['variable']\n",
    "\n",
    "# drop the timesteps column so it doesn't end up as a feature column\n",
    "melt = melt.drop('timesteps', axis=1)\n",
    "\n",
    "# convert the value to a numeric value - not sure why this happened\n",
    "melt['value'] = pd.to_numeric(melt.value)\n",
    "\n",
    "# spread the values out so there's one row per request the columns contain the timesteps\n",
    "one_row_per_request = melt.pivot_table(values='value', index=['request_id'], columns=['variable']).reset_index()\n",
    "\n",
    "# fill NA values for timesteps with zeros\n",
    "one_row_per_request = one_row_per_request.fillna(0)\n",
    "\n",
    "# get the labels and groups for a GroupKFold cross validation.\n",
    "fraud = interac_requests.fraud\n",
    "emails = interac_requests['metadata.email']\n",
    "\n",
    "n_examples = one_row_per_request.shape[0]\n",
    "\n",
    "n_timesteps = (one_row_per_request.shape[1] - 1) / n_features\n",
    "\n",
    "print(\"Exampes:\",n_examples, \"Features:\", n_features, \"Timesteps:\",n_timesteps)\n",
    "\n",
    "one_row_per_request.head()\n",
    "\n",
    "# get the values of the array\n",
    "data = one_row_per_request.drop('request_id', axis=1).values\n",
    "\n",
    "# reshape the data from (n_example, n_features*n_timesteps) to (n_examples, n_timesteps, n_features)\n",
    "data = data.reshape((n_examples, int(n_timesteps), n_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the number of interac requests per fraudulent user\n",
    "for email in emails[fraud == True].unique():\n",
    "    print(email, np.sum(fraud[emails == email] == True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Modeling Code - Doesn't Currently Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pandas import read_csv\n",
    "import math\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot\n",
    "from keras.optimizers import adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data is the 3d matrix\n",
    "X = data\n",
    "\n",
    "# labels are fraud\n",
    "y = fraud\n",
    "\n",
    "# groups for splitting are user emails\n",
    "groups = emails\n",
    "\n",
    "# to keep track of average precision score for each split\n",
    "average_precisions = []\n",
    "\n",
    "# Split the data stratified by user/email address so there's no cross contamination.\n",
    "# number of splits is 3 because there's only 3 fraudulent users\n",
    "group_kfold = GroupKFold(n_splits=3)\n",
    "\n",
    "print('Producing {} splits of the data'.format(group_kfold.get_n_splits(X=X, y=y, groups=emails)))\n",
    "\n",
    "# cross validation\n",
    "for train_index, test_index in group_kfold.split(X=X, y=y, groups=groups):\n",
    "\n",
    "    X_train = X[train_index,:,:]\n",
    "    X_test = X[test_index,:,:]\n",
    "    y_train = y[train_index]\n",
    "    y_test = y[test_index]\n",
    "            \n",
    "    opt = adam(lr=1)\n",
    "\n",
    "    # create and fit the LSTM network\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(10, input_shape=(n_timesteps, n_features), return_sequences=False))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "\n",
    "    history = model.fit(X_train, y_train, epochs=2, validation_data=(X_test, y_test), batch_size=50)\n",
    "    pyplot.plot(history.history['loss'])\n",
    "    pyplot.plot(history.history['val_loss'])\n",
    "    pyplot.title('model train vs validation loss')\n",
    "    pyplot.ylabel('loss')\n",
    "    pyplot.xlabel('epoch')\n",
    "    pyplot.legend(['train', 'validation'], loc='upper right')\n",
    "    pyplot.show()\n",
    "    \n",
    "#     probs_test = model.predict_proba(X_test, )\n",
    "#     print(probs_test)\n",
    "    \n",
    "#     average_precision_score = average_precision_score(y_true=y_test, y_score=probs_test)\n",
    "    \n",
    "#     average_precisions.append(average_precision_score)\n",
    "    \n",
    "# print(\"Mean Average Precision Score:\", np.mean(average_precisions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
