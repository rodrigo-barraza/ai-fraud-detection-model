{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Random Forest Model to Predict Fraudulent Credit Card and Interac Requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for auto-reloading extensions - helpful if you're writing and testing a package\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# for inline plotting in python using matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# for easier plots - also makes matplotlib plots look nicer by default\n",
    "import seaborn as sns\n",
    "\n",
    "# set up for using plotly offline without an API key - great for interactive plots\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "import plotly.graph_objs as go\n",
    "import plotly.figure_factory as ff\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "# for numerical work\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pymongo\n",
    "\n",
    "import datetime\n",
    "import time\n",
    "import json\n",
    "\n",
    "from pandas.io.json import json_normalize\n",
    "from pymongo import MongoClient\n",
    "\n",
    "import pickle\n",
    "\n",
    "from confluent_kafka import Producer\n",
    "\n",
    "import bson\n",
    "from bson import json_util\n",
    "\n",
    "import math\n",
    "\n",
    "from einsteinds import db as edb\n",
    "from einsteinds import event_processing\n",
    "from einsteinds import ml\n",
    "from einsteinds import plots\n",
    "from einsteinds import utils\n",
    "\n",
    "\n",
    "clean_events = event_processing.clean_events\n",
    "\n",
    "# load the database credentials from file\n",
    "with open('../creds/local_creds.json') as json_data:\n",
    "    creds = json.load(json_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview:\n",
    "\n",
    "The training examples for the random forest model are summarized or aggregate features derived from the user's event history in the hour before the request. At a high level, the process to product these summaries is:\n",
    "\n",
    "1. Select a credit card or interac purchase request.\n",
    "2. Get the events in the hour before the request for the user.\n",
    "3. Summarize the events into a set of numerical aggregates.\n",
    "\n",
    "The process to train the random forest (or really any classifier) is to:\n",
    "\n",
    "1. Label each training example as either fraudulent or not by comparing the user emails with the blacklist.\n",
    "2. Find the optimal random forest model by using bayesian optimization combined with an n-fold grouped cross validation split with the training data.\n",
    "3. Save the model and the model features(columns).\n",
    "\n",
    "The process for prediction is as follows:\n",
    "\n",
    "1. Select a credit card or interac purchase request for prediction.\n",
    "2. Generate a single summarized training example.\n",
    "3. Format the training example so that it is consistent with the features used in the trained model.\n",
    "4. Generate a prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processing The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `einsteinds` package we created has a number of methods to clean events, generate sets of events related to a request and generate summaries based on those events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the database with the credentials\n",
    "db = edb.Database(creds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets get all the requests for January and February"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the requests in Janurary in February\n",
    "requests = db.get_deposit_requests(start_date=datetime.datetime(2018,1,1), end_date=datetime.datetime(2018,3,1))\n",
    "\n",
    "requests[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll generate a single request event set based on the request above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "request_set = db.get_deposit_request_set(requests[0])\n",
    "\n",
    "request_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the event data is still in its raw format. Before we summarize the data we want to clean it and make it consistent. The following examples shows the output of a cleaned event. We don't call this directly, but it is happening behind the scenes and the functionality to handle the cleaning is in the`einsteinds` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_processing.clean_events([request_set['events'][0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll summarize that single request set into a request summary. Note that this also handles event cleaning. We wanted to have the cleaning happen as part of the summary creation rather than at the request set stage, as we may want to use the raw request sets for a different purpose later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "request_summary = db.get_summarized_request_sets(rsets=[request_set])\n",
    "\n",
    "request_summary.reset_index().to_dict('records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can handle this process in pieces or do it all at once. The code below, generates all the request sets in January and February."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the requests in Janurary in February\n",
    "rsets = db.get_deposit_request_sets(start_date=datetime.datetime(2018,1,1), end_date=datetime.datetime(2018,3,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can summarize all those request sets, but we could also jump to the end result with the later call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the request summaries based of the request sets\n",
    "summaries = db.get_summarized_request_sets(rsets=rsets)\n",
    "\n",
    "# or do the whole thing at once in one step\n",
    "summaries = db.get_summarized_request_sets(start_date=datetime.datetime(2018,1,1), end_date=datetime.datetime(2018,3,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can add the fraud labels to the data with one call that gets the blacklist from the database, compares the emails in the requests and adds a fraud column to the datafram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries_with_fraud = db.add_fraud_label(summaries, 'user_email')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train an optimized random forest using the summarized data. The function below trains an optimized random forest model using bayesian hyperparameter optimization and grouped n-fold cross validation. The number of folds is dependant on the number of groups in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = ml.generate_optimal_random_forest(summaries_with_fraud)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the parameters of the random forest produced and the resulting dictionary also contains the the features, which need to be saved to generate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets get some new data and generate some predictions. Let's get all the request summaries for the month of April.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = db.get_summarized_request_sets(start_date=datetime.datetime(2018,3,1), end_date=datetime.datetime(2018,4,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to format the data for prediction, which basically amounts to selecting the columns that were used to build the model and ordering them correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ml.prepare_for_prediction(new_data, rf_model['model_features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = new_data.reset_index()[['request_id', 'user_email']]\n",
    "\n",
    "results['fraud'] = rf_model['classifier'].predict(X)\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have also created a simple function to predict from a raw request, that creates the request set, cleans the events, produces the summary, and generates a prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_requests = db.get_deposit_requests(start_date=datetime.datetime(2018,4,1), end_date=datetime.datetime(2018,5,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_request = new_requests[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on a single raw request\n",
    "ml.predict_from_request(request=raw_request, db=db, model=rf_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
