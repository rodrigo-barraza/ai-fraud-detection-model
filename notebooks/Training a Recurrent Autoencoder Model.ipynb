{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for auto-reloading extensions - helpful if you're writing and testing a package\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# for inline plotting in python using matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# for easier plots - also makes matplotlib plots look nicer by default\n",
    "import seaborn as sns\n",
    "\n",
    "# set up for using plotly offline without an API key - great for interactive plots\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "import plotly.graph_objs as go\n",
    "import plotly.figure_factory as ff\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "# for numerical work\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pymongo\n",
    "\n",
    "import datetime\n",
    "import time\n",
    "import json\n",
    "\n",
    "from pandas.io.json import json_normalize\n",
    "from pymongo import MongoClient\n",
    "\n",
    "import pickle\n",
    "\n",
    "from confluent_kafka import Producer\n",
    "\n",
    "import bson\n",
    "from bson import json_util\n",
    "\n",
    "import math\n",
    "\n",
    "from einsteinds import db as edb\n",
    "from einsteinds import event_processing\n",
    "from einsteinds import ml\n",
    "from einsteinds import plots\n",
    "from einsteinds import utils\n",
    "\n",
    "\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, scale\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras import regularizers\n",
    "\n",
    "\n",
    "clean_events = event_processing.clean_events\n",
    "\n",
    "# load the database credentials from file\n",
    "with open('../creds/local_creds.json') as json_data:\n",
    "    creds = json.load(json_data)\n",
    "    \n",
    "db = edb.Database(creds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the request sets for the month of January and February\n",
    "request_sets = db.get_deposit_request_sets(start_date=datetime.datetime(2018,1,1), end_date=datetime.datetime(2018,3,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleans the request event and the previous hour's events in the request sets\n",
    "request_sets_clean = event_processing.clean_request_sets(request_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sets of records that contain only numerical features for feeding into a recurrent neural network\n",
    "recurrent_summaries = event_processing.create_recurrent_request_summaries(request_sets_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten them into just a big list of events\n",
    "recurrent_events = [event for summary in recurrent_summaries for event in summary]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(recurrent_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = interac_data_dict['data']\n",
    "y = interac_data_dict['labels'].values\n",
    "groups = interac_data_dict['users'].values\n",
    "feature_names = interac_data_dict['feature_names']\n",
    "\n",
    "x_shape = X.shape\n",
    "\n",
    "n_examples = x_shape[0]\n",
    "n_timesteps = x_shape[1]\n",
    "n_features = x_shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def balanced_class_group_splits(X, y, groups):\n",
    "\n",
    "    emails = groups\n",
    "    \n",
    "    all_examples = pd.DataFrame({'requests': y, 'users': groups})\n",
    "\n",
    "    frauds = all_examples[all_examples.requests == 1]\n",
    "    not_frauds = all_examples[all_examples.requests == 0]\n",
    "\n",
    "    user_counts = frauds.groupby('users', as_index=False)['requests'].count().sort_values(by='requests', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    display(user_fraud_counts)\n",
    "\n",
    "    print(\"Total Fraudulent requests\", user_counts.requests.sum())\n",
    "    print(\"Most Fraudulent requests by a single user:\", user_counts.requests[0])\n",
    "    print(\"Max Number of Splits\",math.ceil(user_counts.requests.sum()/user_counts.requests[0]))\n",
    "\n",
    "    max_splits = math.ceil(user_counts.requests.sum()/user_counts.requests[0])\n",
    "    max_group_size = math.ceil(user_counts.requests.sum()/max_splits)\n",
    "    groups = {}\n",
    "\n",
    "    print(\"Maximum Group Size\", max_group_size)\n",
    "\n",
    "    split_order = np.arange(max_splits)\n",
    "    splits = split_order + 1\n",
    "    split_counts = np.zeros((max_splits))\n",
    "\n",
    "    for i in range(user_counts.shape[0]):\n",
    "\n",
    "        for split in splits:#[split_counts.argsort()]:\n",
    "            if split not in groups.keys():\n",
    "                groups[split] = {'users': [], 'requests': 0}\n",
    "\n",
    "            current_group_size = groups[split]['requests']\n",
    "            remaining_space = max_group_size - current_group_size\n",
    "\n",
    "            if i == 0:\n",
    "                groups[split]['users'].append(user_counts.users[i])\n",
    "                groups[split]['requests'] += user_counts.requests[i]\n",
    "                groups[split]['user_count'] = len(groups[split]['users'])\n",
    "                break\n",
    "\n",
    "            elif (current_group_size < max_group_size) & (remaining_space >= user_counts.requests[i]):\n",
    "                groups[split]['users'].append(user_counts.users[i])\n",
    "                groups[split]['requests'] += user_counts.requests[i]\n",
    "                groups[split]['user_count'] = len(groups[split]['users'])\n",
    "\n",
    "                #split_counts[split-1] = groups[split]['requests']/len(groups[split]['users'])\n",
    "\n",
    "                break\n",
    "    \n",
    "    positive_groups = groups\n",
    "    \n",
    "    user_counts = not_frauds.groupby('users', as_index=False)['requests'].count().sort_values(by='requests', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    display(user_counts.head())\n",
    "\n",
    "    print(\"Total Not Fraudulent equests\", user_counts.requests.sum())\n",
    "    print(\"Most Requests by a single user:\", user_counts.requests[0])\n",
    "    print(\"Max number of splits\",math.floor(user_counts.requests.sum()/user_counts.requests[0]))\n",
    "\n",
    "    max_group_size = math.ceil(user_counts.requests.sum()/max_splits)\n",
    "    groups = {}\n",
    "\n",
    "    print(\"Maximum Group Size\", max_group_size)\n",
    "\n",
    "    split_order = np.arange(max_splits)\n",
    "    splits = split_order + 1\n",
    "    split_counts = np.zeros((max_splits))\n",
    "\n",
    "    for i in range(user_counts.shape[0]):\n",
    "\n",
    "        for split in splits:#[split_counts.argsort()]:\n",
    "            if split not in groups.keys():\n",
    "                groups[split] = {'users': [], 'requests': 0}\n",
    "\n",
    "            current_group_size = groups[split]['requests']\n",
    "            remaining_space = max_group_size - current_group_size\n",
    "\n",
    "            if i == 0:\n",
    "                groups[split]['users'].append(user_counts.users[i])\n",
    "                groups[split]['requests'] += user_counts.requests[i]\n",
    "                groups[split]['user_count'] = len(groups[split]['users'])\n",
    "                break\n",
    "\n",
    "            elif (current_group_size < max_group_size) & (remaining_space >= user_counts.requests[i]):\n",
    "                groups[split]['users'].append(user_counts.users[i])\n",
    "                groups[split]['requests'] += user_counts.requests[i]\n",
    "                groups[split]['user_count'] = len(groups[split]['users'])\n",
    "\n",
    "                #split_counts[split-1] = groups[split]['requests']/len(groups[split]['users'])\n",
    "\n",
    "                break\n",
    "    \n",
    "    negative_groups = groups\n",
    "    \n",
    "    datasets = []\n",
    "\n",
    "    for split in splits:\n",
    "\n",
    "        # get the lists of positive and negative users\n",
    "        pos_users = positive_groups[split]['users']\n",
    "        neg_users = negative_groups[split]['users']\n",
    "\n",
    "        # indices\n",
    "        indices = np.arange(len(y))\n",
    "\n",
    "        # get the indices of pos and neg user groups\n",
    "        split_positive_indices = indices[np.isin(emails, pos_users)]\n",
    "        split_negative_indices = indices[np.isin(emails, neg_users)]\n",
    "\n",
    "        # split the users \n",
    "        this_split_indices = np.concatenate((split_positive_indices, split_negative_indices), axis=0)\n",
    "\n",
    "        # get the rest of the indices\n",
    "        the_rest_indices =  np.array(list(set(indices) - set(list(this_split_indices))))\n",
    "\n",
    "\n",
    "        # create the training and test sets for this split\n",
    "        y_test = y[this_split_indices]\n",
    "        y_train = y[the_rest_indices]\n",
    "        X_test = X[this_split_indices,:,:]\n",
    "        X_train = X[the_rest_indices,:,:]\n",
    "\n",
    "        # append splits\n",
    "        datasets.append((X_train, X_test, y_train, y_test))\n",
    "        \n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_splits = balanced_class_group_splits(X,y,groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_splits[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraudsters = groups[y==1]\n",
    "list(set(fraudsters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.dtype)\n",
    "print(X.shape)\n",
    "print(y.dtype)\n",
    "print(y.shape)\n",
    "print(type(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pandas import read_csv\n",
    "import math\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot\n",
    "from keras.optimizers import adam\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight, compute_sample_weight\n",
    "\n",
    "\n",
    "compute_class_weight(class_weight='balanced', classes=np.unique(y), y=y)\n",
    "\n",
    "# to keep track of average precision score for each split\n",
    "ys=[]\n",
    "preds=[]\n",
    "probs=[]\n",
    "\n",
    "# Split the data stratified by user/email address so there's no cross contamination.\n",
    "# number of splits is 3 because there's only 3 fraudulent users\n",
    "group_kfold = GroupKFold(n_splits=3)\n",
    "\n",
    "print('Producing {} splits of the data'.format(group_kfold.get_n_splits(X=X, y=y, groups=groups)))\n",
    "\n",
    "# cross validation\n",
    "#for train_index, test_index in group_kfold.split(X=X, y=y, groups=groups):\n",
    "for X_train, X_test, y_train, y_test in data_splits:\n",
    "\n",
    "#     X_train = X[train_index,:,:]\n",
    "#     X_test = X[test_index,:,:]\n",
    "#     y_train = y[train_index]\n",
    "#     y_test = y[test_index]\n",
    "    print(X_train.shape, X_train.dtype)\n",
    "    print(X_test.shape, X_test.dtype)\n",
    "    print(y_train.shape, y_train.dtype)\n",
    "    print(y_test.shape, y_test.dtype)\n",
    "    print('Train Fraud: ', len(y_train[y_train == 1])/len(y_train))\n",
    "    #print('Train Fraud Emails: ', list(set(groups[train_index][y_train == 1])))\n",
    "    print('Test Fraud: ', len(y_test[y_test == 1])/len(y_test))\n",
    "    #print('Test Fraud Emails: ', list(set(groups[test_index][y_test == 1])), '\\n')\n",
    "    print(list(set(groups[train_index]) & set(groups[test_index]) & set(groups[y == 1])))\n",
    "\n",
    "    class_weight = dict(enumerate(compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)))\n",
    "            \n",
    "    opt = adam(lr=1)\n",
    "\n",
    "    # create and fit the LSTM network\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(20, input_shape=(n_timesteps, n_features), return_sequences=False, go_backwards=True))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "\n",
    "    history = model.fit(X_train, y_train, epochs=5, validation_data=(X_test, y_test), batch_size=50, class_weight=class_weight)\n",
    "    pyplot.plot(history.history['loss'])\n",
    "    pyplot.plot(history.history['val_loss'])\n",
    "    pyplot.title('model train vs validation loss')\n",
    "    pyplot.ylabel('loss')\n",
    "    pyplot.xlabel('epoch')\n",
    "    pyplot.legend(['train', 'validation'], loc='upper right')\n",
    "    pyplot.show()\n",
    "    \n",
    "    ys.append(y_test)\n",
    "    preds.append(model.predict(X_test))\n",
    "    probs.append(model.predict_proba(X_test))\n",
    "\n",
    "\n",
    "\n",
    "avps = []\n",
    "\n",
    "for i in range(len(ys)):\n",
    "    avps.append(average_precision_score(ys[i], preds[i]))\n",
    "    print(confusion_matrix(ys[i], preds[i]>0.6))\n",
    "    \n",
    "np.mean(avps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(20, input_shape=(n_timesteps, n_features), return_sequences=False))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "history = model.fit(X, y, epochs=10, batch_size=50, class_weight=dict(enumerate(compute_class_weight())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.astype('float32')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = Sequential()\n",
    "autoencoder.add(LSTM(5, input_shape=(n_timesteps, n_features), return_sequences=True))\n",
    "autoencoder.add(LSTM(n_features, return_sequences=True))\n",
    "autoencoder.compile(loss='mean_squared_error', optimizer='RMSprop')\n",
    "history = autoencoder.fit(X,X, epochs=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = autoencoder.predict(X)\n",
    "\n",
    "squared_error = np.square(X - preds)\n",
    "\n",
    "total_squared_error = np.apply_over_axes(np.sum, a=squared_error, axes=[1,2])\n",
    "total_squared_error = total_squared_error.reshape((1777))\n",
    "\n",
    "results = pd.DataFrame({'email': groups, 'anomaly_score': total_squared_error, 'fraud': y})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.sort_values(by='anomaly_score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_results = results.groupby('email', as_index=False)[['anomaly_score','fraud']].agg({'anomaly_score': ['mean','sum', 'median'], 'fraud': ['sum']})\n",
    "\n",
    "user_results.columns = [col[0] if col[1] == '' else col[0]+'_'+col[1] for col in user_results.columns.ravel()]\n",
    "\n",
    "user_results = user_results.sort_values(by=['anomaly_score_sum'], ascending=False)\n",
    "\n",
    "user_results.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
    "sns.distplot(results.anomaly_score, hist=False, rug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
    "sns.distplot(user_results.anomaly_score_sum, hist=False, rug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
    "sns.distplot(user_results.anomaly_score_mean, hist=False, rug=True)\n",
    "\n",
    "user_results.sort_values(by='anomaly_score_median', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "from pandas.io.json import json_normalize\n",
    "\n",
    "# load the database credentials from file\n",
    "with open('../user_aggregation_pipeline//creds.json') as json_data:\n",
    "    creds = json.load(json_data)\n",
    "\n",
    "def get_user_events(user_email):\n",
    "    \n",
    "    # set up a database with credentials\n",
    "    client = MongoClient(creds['connection_string'])\n",
    "\n",
    "    # get the full history of interac requests\n",
    "    events = list(client['production']['eventCollection'].find({\n",
    "            'metadata.email': user_email}))\n",
    "    \n",
    "    df = pd.DataFrame(json_normalize(events))\n",
    "    df = df.sort_values(by='created', ascending=False)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def display_all_user_events(user_email):\n",
    "\n",
    "    df = get_user_events(user_email)\n",
    "\n",
    "    with pd.option_context('display.max_rows', 1000):\n",
    "\n",
    "        display(df[['created','eventCategory','eventAction','eventLabel','metadata.amount','value','metadata.ip']])\n",
    "\n",
    "display_all_user_events('david9074@gmail.com')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('--------------------------------------')\n",
    "print('Total Events = '+str(len(y)))\n",
    "print('--------------------------------------')\n",
    "i = 1    \n",
    "    \n",
    "for train_index, test_index in group_kfold.split(X=X, y=y, groups=groups):\n",
    "    print('======================================================================')\n",
    "    print('SPLIT:',i)\n",
    "    print('------------')\n",
    "    i += 1\n",
    "    \n",
    "    # Do training set/test set.\n",
    "    for idx_set in [train_index,test_index]:\n",
    "        if set(idx_set)==set(train_index):\n",
    "            print('======================================================================')\n",
    "            print('TRAINING SET:')\n",
    "            print('------------')\n",
    "        else:\n",
    "            print('--------------------------------------')\n",
    "            print('TEST SET:')\n",
    "            print('--------')\n",
    "        Email = groups[idx_set]\n",
    "        Fraud = y[idx_set]\n",
    "        \n",
    "        df = pd.concat([Email,Fraud],axis=1)\n",
    "        df.columns=['emails','fraud']\n",
    "    \n",
    "        print('Event Count:')\n",
    "        print( pd.DataFrame( df.groupby(['fraud']).size(), columns=['Count'] ) )\n",
    "        print(' ')\n",
    "\n",
    "        q = df[df.fraud==True].groupby('emails').size()\n",
    "        print('Number unique FRAUD emails: '+str(len(q)))\n",
    "        print('--------------------------')\n",
    "        print(q)\n",
    "    \n",
    "        q = df[df.fraud==False].groupby('emails').size()\n",
    "        print('--------------------------------------')\n",
    "        print('Number unique VALID emails: '+str(len(q)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud = interac_data_dict['labels']\n",
    "emails = interac_data_dict['users']\n",
    "\n",
    "type(emails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_values = interac_data_dict['labels'].values\n",
    "email_values = interac_data_dict['users'].values\n",
    "y = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('--------------------------------------')\n",
    "print('Total Events = '+str(len(y)))\n",
    "print('--------------------------------------')\n",
    "    \n",
    "for train_index, test_index in group_kfold.split(X=X, y=y, groups=groups):\n",
    "    # Do training set/test set.\n",
    "    for idx_set in [train_index,test_index]:\n",
    "        if set(idx_set)==set(train_index):\n",
    "            print('======================================================================')\n",
    "            print('TRAINING SET:')\n",
    "            print('------------')\n",
    "        else:\n",
    "            print('--------------------------------------')\n",
    "            print('TEST SET:')\n",
    "            print('--------')\n",
    "        Email = emails.iloc[idx_set]\n",
    "        Fraud = fraud.iloc[idx_set]\n",
    "        \n",
    "        df = pd.concat([Email,Fraud],axis=1)\n",
    "        df.columns=['emails','fraud']\n",
    "    \n",
    "        print('Event Count:')\n",
    "        print( pd.DataFrame( df.groupby(['fraud']).size(), columns=['Count'] ) )\n",
    "        print(' ')\n",
    "\n",
    "        q = df[df.fraud==True].groupby('emails').size()\n",
    "        print('Number unique FRAUD emails: '+str(len(q)))\n",
    "        print('--------------------------')\n",
    "        print(q)\n",
    "    \n",
    "        q = df[df.fraud==False].groupby('emails').size()\n",
    "        print('--------------------------------------')\n",
    "        print('Number unique VALID emails: '+str(len(q)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
